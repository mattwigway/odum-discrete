[
  {
    "objectID": "discrete-choice.html#about-me",
    "href": "discrete-choice.html#about-me",
    "title": "Introduction to Discrete Choice Models",
    "section": "About me",
    "text": "About me\n\nAssistant professor of City and Regional Planning\nResearch focus: transportation modeling and simulation\n\nHeavy use of discrete choice models\n\nPhD in Geography from Arizona State\nThree years experience as transportation modeling software developer"
  },
  {
    "objectID": "discrete-choice.html#before-we-go-any-further",
    "href": "discrete-choice.html#before-we-go-any-further",
    "title": "Introduction to Discrete Choice Models",
    "section": "Before we go any further",
    "text": "Before we go any further\n\nInstall Python\nInstall necessary Python packages using pip: biogeme, statsmodels, pandas, numpy, scipy\nSlides, code, and data at https://projects.indicatrix.org/odum-discrete/"
  },
  {
    "objectID": "discrete-choice.html#what-are-discrete-choice-models",
    "href": "discrete-choice.html#what-are-discrete-choice-models",
    "title": "Introduction to Discrete Choice Models",
    "section": "What are discrete choice models?",
    "text": "What are discrete choice models?\n\nRegression models of discrete outcomes\nMost are consistent with random utility theory, an theory of economically rational decisionmaking"
  },
  {
    "objectID": "discrete-choice.html#what-are-discrete-choice-models-used-for",
    "href": "discrete-choice.html#what-are-discrete-choice-models-used-for",
    "title": "Introduction to Discrete Choice Models",
    "section": "What are discrete choice models used for?",
    "text": "What are discrete choice models used for?\n\nEconomic analysis\n\nParticularly willingness-to-pay/valuation\n\nMarketing\nHealthcare\nTransportation planning"
  },
  {
    "objectID": "discrete-choice.html#the-simplest-discrete-choice-model-binary-logistic-regression",
    "href": "discrete-choice.html#the-simplest-discrete-choice-model-binary-logistic-regression",
    "title": "Introduction to Discrete Choice Models",
    "section": "The simplest discrete choice model: binary logistic regression",
    "text": "The simplest discrete choice model: binary logistic regression\n\nYou may have also just heard this called “logistic regression”\nIf you’ve taken regression you’re probably familiar with this model\nIt is used for binary (two outcome) variables, e.g.\n\nfraudulent or not\nwork in person or work at home\nin foreclosure or not\netc."
  },
  {
    "objectID": "discrete-choice.html#the-binary-logistic-regression-model-conceptually",
    "href": "discrete-choice.html#the-binary-logistic-regression-model-conceptually",
    "title": "Introduction to Discrete Choice Models",
    "section": "The binary logistic regression model: conceptually",
    "text": "The binary logistic regression model: conceptually\n\nWe are never going to be 100% sure of one outcome or the other, so we want to model the probability of the outcomes\nWe could just assign 1 to one outcome and 0 to the other, and run a linear regression model\nThis is the linear probability model"
  },
  {
    "objectID": "discrete-choice.html#the-problem-with-the-linear-probability-model",
    "href": "discrete-choice.html#the-problem-with-the-linear-probability-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "The problem with the linear probability model",
    "text": "The problem with the linear probability model\n\nPredicted values are not constrained between 0 and 1"
  },
  {
    "objectID": "discrete-choice.html#binary-logistic-regression",
    "href": "discrete-choice.html#binary-logistic-regression",
    "title": "Introduction to Discrete Choice Models",
    "section": "Binary logistic regression",
    "text": "Binary logistic regression\n\nInstead of predicting the probability, we predict the log-odds (aka logit)"
  },
  {
    "objectID": "discrete-choice.html#binary-logistic-regression-the-math",
    "href": "discrete-choice.html#binary-logistic-regression-the-math",
    "title": "Introduction to Discrete Choice Models",
    "section": "Binary logistic regression: the math",
    "text": "Binary logistic regression: the math\n\\[\ny^* = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 \\cdots + \\epsilon\n\\]\n\\[\n\\epsilon \\thicksim \\mathrm{Logistic}\n\\] \\[\np(y = 1) = p(y^* &gt; 0) = \\frac{e^{y^*}}{1 + e^{y^*}}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#interpreting-a-binary-logistic-regression-model",
    "href": "discrete-choice.html#interpreting-a-binary-logistic-regression-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting a binary logistic regression model",
    "text": "Interpreting a binary logistic regression model\n\n\n\n\n\n\nCoefficient\nStd. Err.\n\\(t\\)\np\n\n\n\n\nIntercept\n0.726\n0.247\n2.944\n0.003\n\n\nAge\n-0.023\n0.005\n-5.065\n0\n\n\nCollege degree\n0.927\n0.131\n7.102\n0\n\n\nMale\n0.252\n0.128\n1.967\n0.049\n\n\nn\n\n\n\n1171\n\n\nLog-likelihood\n\n\n\n-755.397\n\n\nLog-likelihood at constants\n\n\n\n-798.247\n\n\nPseudo-R²\n\n\n\n0.0536795\n\n\n\n\n\n\nAre older people more likely to be able to WFH?\nAre college-educated people?\nAre men?"
  },
  {
    "objectID": "discrete-choice.html#statistical-significance-and-p-values",
    "href": "discrete-choice.html#statistical-significance-and-p-values",
    "title": "Introduction to Discrete Choice Models",
    "section": "Statistical significance and p-values",
    "text": "Statistical significance and p-values\n\nCoefficients are approximations to population-level parameters, and contain sampling error because we haven’t sampled the entire population\nThe standard error (\\(\\sigma\\)) is a measure of how precise the coefficient is\nWe assume coefficient estimates are asymtotically normal\nBased on a normal distribution, there is a 95% chance the population parameter is within \\(1.96 \\times \\sigma\\) of the estimate"
  },
  {
    "objectID": "discrete-choice.html#statistical-significance-and-p-values-1",
    "href": "discrete-choice.html#statistical-significance-and-p-values-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Statistical significance and p-values",
    "text": "Statistical significance and p-values\n\nWe can also perform a hypothesis test that the coefficient is different from 0\nTo do this, we compute a \\(z\\)-value, which is just the coefficient divided by the standard error\nPut another way, the \\(z\\)-value is the number of standard errors between the coefficient and zero\nLarge coefficients lead to large \\(z\\)-values\nLarge standard errors (i.e. high uncertainty) lead to small \\(z\\)-values"
  },
  {
    "objectID": "discrete-choice.html#statistical-significance-and-p-values-2",
    "href": "discrete-choice.html#statistical-significance-and-p-values-2",
    "title": "Introduction to Discrete Choice Models",
    "section": "Statistical significance and p-values",
    "text": "Statistical significance and p-values\n\n\\(z\\) values larger in magnitude than 1.96 indicate that there is less than a 5% chance of observing a coefficient as larger or larger than the estimated coefficient if the true coefficient were zero"
  },
  {
    "objectID": "discrete-choice.html#statistical-significance-and-p-values-3",
    "href": "discrete-choice.html#statistical-significance-and-p-values-3",
    "title": "Introduction to Discrete Choice Models",
    "section": "Statistical significance and p-values",
    "text": "Statistical significance and p-values\n\nThe \\(p\\)-value is the probability of observing a coefficient as large or larger in magnitude, if the true coefficient were zero\nComputed using the normal cumulative distribution function\nUsual rule is that \\(p &lt; 0.05\\) is statistically significant, but in larger datasets \\(p &lt; 0.01\\) or even \\(p &lt; 0.001\\) may be used\nFor more information about the math, see the hypothesis testing section of any statistics textbook\nLogistic regression hypothesis tests are computed just like linear regression, except using the normal instead of \\(t\\) distribution"
  },
  {
    "objectID": "discrete-choice.html#odds-ratios",
    "href": "discrete-choice.html#odds-ratios",
    "title": "Introduction to Discrete Choice Models",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\n\n\n\nCoefficient\nStd. Err.\n\\(t\\)\np\n\n\n\n\nIntercept\n0.726\n0.247\n2.944\n0.003\n\n\nAge\n-0.023\n0.005\n-5.065\n0\n\n\nCollege degree\n0.927\n0.131\n7.102\n0\n\n\nMale\n0.252\n0.128\n1.967\n0.049\n\n\nn\n\n\n\n1171\n\n\nLog-likelihood\n\n\n\n-755.397\n\n\nLog-likelihood at constants\n\n\n\n-798.247\n\n\nPseudo-R²\n\n\n\n0.0536795\n\n\n\n\n\n\nWhat does a “0.93 increase in log-odds” mean anyways?\nBecause this is hard to interpret, we often use an odds ratio instead\nThis is just the exponentiation of the coefficient"
  },
  {
    "objectID": "discrete-choice.html#interpreting-odds-ratios-i",
    "href": "discrete-choice.html#interpreting-odds-ratios-i",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting odds ratios I",
    "text": "Interpreting odds ratios I\n\n\n\n\n\n\nOdds ratio\n\\(t\\)\np\n\n\n\n\nIntercept\n2.067\n2.944\n0.003\n\n\nAge\n0.977\n-5.065\n0\n\n\nCollege degree\n2.527\n7.102\n0\n\n\nMale\n1.287\n1.967\n0.049\n\n\nn\n\n\n1171\n\n\nLog-likelihood\n\n\n-755.397\n\n\nLog-likelihood at constants\n\n\n-798.247\n\n\nPseudo-R²\n\n\n0.0536795\n\n\n\n\n\n\nOdds ratios measure the change in the odds of an event happening\n\nOdds is the probability of something happening divided by the probability of it not happening\ne.g. 5 to 1 odds means that out of 6 trials something will happen 5 times on average\n\nThey are multiplicative, so an odds ratio of 1.287 means that men have 28.7% higher odds of being able to work from home"
  },
  {
    "objectID": "discrete-choice.html#interpreting-odds-ratios-ii",
    "href": "discrete-choice.html#interpreting-odds-ratios-ii",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting odds ratios II",
    "text": "Interpreting odds ratios II\n\n\n\n\n\n\nOdds ratio\n\\(t\\)\np\n\n\n\n\nIntercept\n2.067\n2.944\n0.003\n\n\nAge\n0.977\n-5.065\n0\n\n\nCollege degree\n2.527\n7.102\n0\n\n\nMale\n1.287\n1.967\n0.049\n\n\nn\n\n\n1171\n\n\nLog-likelihood\n\n\n-755.397\n\n\nLog-likelihood at constants\n\n\n-798.247\n\n\nPseudo-R²\n\n\n0.0536795\n\n\n\n\n\n\nWhat does an odds ratio of 0.977 mean?\n\nEach additional year of age is associated with 2.3% lower odds of being able to work from home"
  },
  {
    "objectID": "discrete-choice.html#odds-ratios-are-not-risk-ratios",
    "href": "discrete-choice.html#odds-ratios-are-not-risk-ratios",
    "title": "Introduction to Discrete Choice Models",
    "section": "Odds ratios are not risk ratios",
    "text": "Odds ratios are not risk ratios\n\nOdds ratios are often misinterpreted as risk ratios, the percent change in the probability\nMen have 28.7% higher odds of being able to work from home, not 28.7% higher probability\n\nMany people say something like 28.7% more likely, but this is unclear that it refers to odds rather than probability"
  },
  {
    "objectID": "discrete-choice.html#running-a-logistic-regression-in-python",
    "href": "discrete-choice.html#running-a-logistic-regression-in-python",
    "title": "Introduction to Discrete Choice Models",
    "section": "Running a logistic regression in Python",
    "text": "Running a logistic regression in Python\n\n\n\nlogistic_basic.py\n\nimport pandas as pd, statsmodels.api as sm\n\ndata = pd.read_csv(\"data/wfh_prediction_covidfuture.csv\")\n\nmodel = sm.Logit(\n    data.wfh_expectation, # dependent variables\n    sm.add_constant( # add an intercept to the dataframe\n        pd.get_dummies( # convert gender to a dummy variable\n            data[[\"age\", \"gender\", \"college\"]],\n        )\n        .drop(columns=[\"gender_Female\"]) # drop one category from dummy variable\n        .astype(\"float64\") # convert everything to numeric\n    )\n)\n\nresult = model.fit()\nprint(result.summary())"
  },
  {
    "objectID": "discrete-choice.html#running-a-logistic-regression-in-python-results",
    "href": "discrete-choice.html#running-a-logistic-regression-in-python-results",
    "title": "Introduction to Discrete Choice Models",
    "section": "Running a logistic regression in Python: results",
    "text": "Running a logistic regression in Python: results\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nwfh_expectation\nNo. Observations:\n1171\n\n\nModel:\nLogit\nDf Residuals:\n1167\n\n\nMethod:\nMLE\nDf Model:\n3\n\n\nDate:\nTue, 30 Sep 2025\nPseudo R-squ.:\n0.05368\n\n\nTime:\n11:18:23\nLog-Likelihood:\n-755.40\n\n\nconverged:\nTrue\nLL-Null:\n-798.25\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.837e-18\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.7258\n0.247\n2.944\n0.003\n0.243\n1.209\n\n\nage\n-0.0230\n0.005\n-5.065\n0.000\n-0.032\n-0.014\n\n\ncollege\n0.9273\n0.131\n7.102\n0.000\n0.671\n1.183\n\n\ngender_Male\n0.2520\n0.128\n1.967\n0.049\n0.001\n0.503"
  },
  {
    "objectID": "discrete-choice.html#exercise-add-the-black-and-hispanic-variables-to-the-model",
    "href": "discrete-choice.html#exercise-add-the-black-and-hispanic-variables-to-the-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: add the black and hispanic variables to the model",
    "text": "Exercise: add the black and hispanic variables to the model\n\n\n\nlogistic_black_hispanic.py\n\nimport pandas as pd, statsmodels.api as sm\n\ndata = pd.read_csv(\"data/wfh_prediction_covidfuture.csv\")\n\nmodel = sm.Logit(\n    data.wfh_expectation, # dependent variables\n    sm.add_constant( # add an intercept to the dataframe\n        pd.get_dummies( # convert gender to a dummy variable\n            data[[\"age\", \"gender\", \"college\", \"black\", \"hispanic\"]],\n        )\n        .drop(columns=[\"gender_Female\"]) # drop one category from dummy variable\n        .astype(\"float64\") # convert everything to numeric\n    )\n)\n\nresult = model.fit()\nprint(result.summary())"
  },
  {
    "objectID": "discrete-choice.html#exercise-add-the-black-and-hispanic-variables-to-the-model-result",
    "href": "discrete-choice.html#exercise-add-the-black-and-hispanic-variables-to-the-model-result",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: add the black and hispanic variables to the model: result",
    "text": "Exercise: add the black and hispanic variables to the model: result\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nwfh_expectation\nNo. Observations:\n1171\n\n\nModel:\nLogit\nDf Residuals:\n1165\n\n\nMethod:\nMLE\nDf Model:\n5\n\n\nDate:\nTue, 30 Sep 2025\nPseudo R-squ.:\n0.05437\n\n\nTime:\n11:18:23\nLog-Likelihood:\n-754.85\n\n\nconverged:\nTrue\nLL-Null:\n-798.25\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n3.164e-17\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n0.7721\n0.261\n2.962\n0.003\n0.261\n1.283\n\n\nage\n-0.0237\n0.005\n-5.072\n0.000\n-0.033\n-0.015\n\n\ncollege\n0.9245\n0.131\n7.056\n0.000\n0.668\n1.181\n\n\nblack\n0.0801\n0.232\n0.346\n0.729\n-0.374\n0.534\n\n\nhispanic\n-0.1971\n0.201\n-0.979\n0.327\n-0.592\n0.197\n\n\ngender_Male\n0.2546\n0.128\n1.986\n0.047\n0.003\n0.506\n\n\n\n\n\n\nIs either statistically significant?"
  },
  {
    "objectID": "discrete-choice.html#an-alternate-conceptualization-of-the-logistic-regression-model-random-utility-theory",
    "href": "discrete-choice.html#an-alternate-conceptualization-of-the-logistic-regression-model-random-utility-theory",
    "title": "Introduction to Discrete Choice Models",
    "section": "An alternate conceptualization of the logistic regression model: random utility theory",
    "text": "An alternate conceptualization of the logistic regression model: random utility theory\n\nRandom utility theory posits that choices have “utility” or value\nWhen faced with a set of options, individuals will choose the option with the highest utility\nWe don’t observe everything that goes into utility, so there is random error"
  },
  {
    "objectID": "discrete-choice.html#logistic-regression-as-a-random-utility-model",
    "href": "discrete-choice.html#logistic-regression-as-a-random-utility-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Logistic regression as a random utility model",
    "text": "Logistic regression as a random utility model\n\nWe define the systematic utility (i.e. no random error) of one alternative (e.g. working from home) as V: \\[\nV_{wfh} = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 \\cdots\n\\]\n\nThe full utility U has an error term:\n\\[\nU_{wfh} = V_{wfh} + \\epsilon_{wfh}\n\\]\n\nOnly differences in utility are meaningful, so we can arbitrarily define the systematic portion of the utility of the other option as zero\n\n\\[\nU_{notwfh} = 0 + \\epsilon_{notwfh}\n\\]\nThe probability of working from home then becomes\n\\[\nP\\left(U_{wfh} &gt; U_{notwfh}\\right)\n\\]"
  },
  {
    "objectID": "discrete-choice.html#error-term-distribution",
    "href": "discrete-choice.html#error-term-distribution",
    "title": "Introduction to Discrete Choice Models",
    "section": "Error term distribution",
    "text": "Error term distribution\n\nWe assume the \\(\\epsilon\\)’s are Gumbel-distributed\nThe Gumbel distribution is an approximation of the normal distribution with some attractive properties\n\nMore on this later\n\nThe important property at the moment is that the difference of two Gumbel-distributed variables is Logistic-distributed\n\nJust like the error term in our logistic regression model"
  },
  {
    "objectID": "discrete-choice.html#showing-the-equivalence-to-the-binary-logistic-regression-model-i",
    "href": "discrete-choice.html#showing-the-equivalence-to-the-binary-logistic-regression-model-i",
    "title": "Introduction to Discrete Choice Models",
    "section": "Showing the equivalence to the binary logistic regression model I",
    "text": "Showing the equivalence to the binary logistic regression model I\nWe have\n\\[\nP\\left(U_{wfh} &gt; U_{notwfh}\\right)\n\\]\n\nwhich we can re-write as\n\\[\nP\\left(V_{wfh} + \\epsilon_{wfh} - (0 + \\epsilon_{notwfh}) &gt; 0\\right)\n\\]"
  },
  {
    "objectID": "discrete-choice.html#showing-the-equivalence-to-the-binary-logistic-regression-model-ii",
    "href": "discrete-choice.html#showing-the-equivalence-to-the-binary-logistic-regression-model-ii",
    "title": "Introduction to Discrete Choice Models",
    "section": "Showing the equivalence to the binary logistic regression model II",
    "text": "Showing the equivalence to the binary logistic regression model II\n\nSince the difference of our Gumbel-distributed error terms is Logistic-distributed, we have\n\n\\[\nP\\left(V_{wfh} + \\epsilon_{\\Delta} &gt; 0\\right)\n\\]\nwith \\(\\epsilon_\\Delta\\) Logistic-distributed, which is the same as our formula for logistic regression."
  },
  {
    "objectID": "discrete-choice.html#estimating-our-binary-logistic-regression-as-a-random-utility-model",
    "href": "discrete-choice.html#estimating-our-binary-logistic-regression-as-a-random-utility-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimating our binary logistic regression as a random utility model",
    "text": "Estimating our binary logistic regression as a random utility model\n\nWe’re going to specify our logistic regression model as a random utility model\nWe’re going to use the Biogeme package to do so"
  },
  {
    "objectID": "discrete-choice.html#estimation-code",
    "href": "discrete-choice.html#estimation-code",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimation code",
    "text": "Estimation code\n\n\n\nbiogeme_binary.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/wfh_prediction_covidfuture.csv\") # read data\ndata = db.Database(\"WFH\", \n    pd.get_dummies(df)\n        .drop(columns=\"gender_Female\")\n        .astype(\"float64\")\n) # convert to Biogeme format\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nalpha = Beta(\"Intercept\", 0, None, None, 0)\nb_age = Beta(\"b_age\", 0, None, None, 0)\nb_college = Beta(\"b_college\", 0, None, None, 0)\nb_male = Beta(\"b_male\", 0, None, None, 0)\n\n# and specify the variables we want to use\nage = Variable(\"age\")\ncollege = Variable(\"college\")\nmale = Variable(\"gender_Male\")\n\n# specify utility functions\nV = {\n    # outcome 1 is WFH\n    1: (alpha + b_age * age + b_college * college + b_male * male),\n    # outcome 0 is non-WFH\n    0: 0\n}\n\n# we also need to define the availability (more on this later)\nav = {0: 1, 1: 1}\n\n# now, set up what type of model we want (logit), what the utility functions are,\n# what the availability variables are, and what the dependent variable is\nlogprob = models.loglogit(V, av, Variable(\"wfh_expectation\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"biogeme_binary\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())"
  },
  {
    "objectID": "discrete-choice.html#result",
    "href": "discrete-choice.html#result",
    "title": "Introduction to Discrete Choice Models",
    "section": "Result",
    "text": "Result\n\n\nResults for model biogeme_binary\nNbr of parameters:      4\nSample size:            1171\nExcluded data:          0\nNull log likelihood:        -811.6753\nFinal log likelihood:       -755.3974\nLikelihood ratio test (null):       112.5559\nRho square (null):          0.0693\nRho bar square (null):          0.0644\nAkaike Information Criterion:   1518.795\nBayesian Information Criterion: 1539.057\n\n        Name     Value  Robust std err.  Robust t-stat.  Robust p-value\n0  Intercept  0.725756         0.246210        2.947712    3.201357e-03\n1      b_age -0.023037         0.004531       -5.084153    3.692710e-07\n2  b_college  0.927291         0.130311        7.115971    1.111333e-12\n3     b_male  0.251951         0.127537        1.975508    4.821053e-02"
  },
  {
    "objectID": "discrete-choice.html#how-does-the-result-compare-to-the-original-model-we-estimated",
    "href": "discrete-choice.html#how-does-the-result-compare-to-the-original-model-we-estimated",
    "title": "Introduction to Discrete Choice Models",
    "section": "How does the result compare to the original model we estimated?",
    "text": "How does the result compare to the original model we estimated?\n\n\n\n\n\n\nCoefficient\nStd. Err.\n\\(t\\)\np\nOdds ratio\n\n\n\n\nIntercept\n0.726\n0.247\n2.944\n0.003\n2.067\n\n\nAge\n-0.023\n0.005\n-5.065\n0\n0.977\n\n\nCollege degree\n0.927\n0.131\n7.102\n0\n2.527\n\n\nMale\n0.252\n0.128\n1.967\n0.049\n1.287\n\n\nn\n\n\n\n1171\n\n\n\nLog-likelihood\n\n\n\n-755.397\n\n\n\nLog-likelihood at constants\n\n\n\n-798.247\n\n\n\nPseudo-R²\n\n\n\n0.0536795\n\n\n\n\n\n\n\nIt should be the same"
  },
  {
    "objectID": "discrete-choice.html#detailed-result",
    "href": "discrete-choice.html#detailed-result",
    "title": "Introduction to Discrete Choice Models",
    "section": "Detailed result",
    "text": "Detailed result\nStored in “biogeme_binary.html”, or some variation—look for most recent."
  },
  {
    "objectID": "discrete-choice.html#exercise",
    "href": "discrete-choice.html#exercise",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise",
    "text": "Exercise\n\nAdd the black and hispanic variables to the model, confirm you get the same results you did before"
  },
  {
    "objectID": "discrete-choice.html#exercise-result",
    "href": "discrete-choice.html#exercise-result",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: result",
    "text": "Exercise: result\n\n\n\nbiogeme_binary_race.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/wfh_prediction_covidfuture.csv\") # read data\ndata = db.Database(\"WFH\", \n    pd.get_dummies(df)\n        .drop(columns=\"gender_Female\")\n        .astype(\"float64\")\n) # convert to Biogeme format\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nalpha = Beta(\"Intercept\", 0, None, None, 0)\nb_age = Beta(\"b_age\", 0, None, None, 0)\nb_college = Beta(\"b_college\", 0, None, None, 0)\nb_male = Beta(\"b_male\", 0, None, None, 0)\nb_black = Beta(\"b_black\", 0, None, None, 0)\nb_hispanic = Beta(\"b_hispanic\", 0, None, None, 0)\n\n# and specify the variables we want to use\nage = Variable(\"age\")\ncollege = Variable(\"college\")\nmale = Variable(\"gender_Male\")\nblack = Variable(\"black\")\nhispanic = Variable(\"hispanic\")\n\n# specify utility functions\nV = {\n    # outcome 1 is WFH\n    1: (alpha + b_age * age + b_college * college + b_male * male +\n        b_black * black + b_hispanic * hispanic),\n    # outcome 0 is non-WFH\n    0: 0\n}\n\n# we also need to define the availability (more on this later)\nav = {0: 1, 1: 1}\n\n# now, set up what type of model we want (logit), what the utility functions are,\n# what the availability variables are, and what the dependent variable is\nlogprob = models.loglogit(V, av, Variable(\"wfh_expectation\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"biogeme_binary_race\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model biogeme_binary_race\nNbr of parameters:      6\nSample size:            1171\nExcluded data:          0\nNull log likelihood:        -811.6753\nFinal log likelihood:       -754.8494\nLikelihood ratio test (null):       113.6518\nRho square (null):          0.07\nRho bar square (null):          0.0626\nAkaike Information Criterion:   1521.699\nBayesian Information Criterion: 1552.093\n\n         Name     Value  Robust std err.  Robust t-stat.  Robust p-value\n0   Intercept  0.772060         0.258984        2.981108    2.872076e-03\n1       b_age -0.023654         0.004630       -5.108796    3.242180e-07\n2   b_college  0.924481         0.130676        7.074590    1.499023e-12\n3      b_male  0.254620         0.127658        1.994551    4.609182e-02\n4     b_black  0.080132         0.232280        0.344981    7.301088e-01\n5  b_hispanic -0.197120         0.197385       -0.998655    3.179621e-01"
  },
  {
    "objectID": "discrete-choice.html#results",
    "href": "discrete-choice.html#results",
    "title": "Introduction to Discrete Choice Models",
    "section": "Results",
    "text": "Results\n\n\nResults for model biogeme_binary_race\nNbr of parameters:      6\nSample size:            1171\nExcluded data:          0\nNull log likelihood:        -811.6753\nFinal log likelihood:       -754.8494\nLikelihood ratio test (null):       113.6518\nRho square (null):          0.07\nRho bar square (null):          0.0626\nAkaike Information Criterion:   1521.699\nBayesian Information Criterion: 1552.093\n\n         Name     Value  Robust std err.  Robust t-stat.  Robust p-value\n0   Intercept  0.772060         0.258984        2.981108    2.872076e-03\n1       b_age -0.023654         0.004630       -5.108796    3.242180e-07\n2   b_college  0.924481         0.130676        7.074590    1.499023e-12\n3      b_male  0.254620         0.127658        1.994551    4.609182e-02\n4     b_black  0.080132         0.232280        0.344981    7.301088e-01\n5  b_hispanic -0.197120         0.197385       -0.998655    3.179621e-01"
  },
  {
    "objectID": "discrete-choice.html#moving-beyond-binary-outcomes-the-multinomial-logit-model",
    "href": "discrete-choice.html#moving-beyond-binary-outcomes-the-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Moving beyond binary outcomes: the multinomial logit model",
    "text": "Moving beyond binary outcomes: the multinomial logit model\n\nOften, we have more than two outcomes\n\nChoice of travel mode might be drive alone, carpool, transit, walk, or bike.\nChoice of Internet service might be DSL, cable, satellite, or fiber\nChoice of vacation destination might be beach, mountains, or theme park\nChoice of treatment for pain might be surgery, physical therapy, non-opioid medication, opioid medication, or accupuncture\n\nWe refer to these outcomes as “alternatives”"
  },
  {
    "objectID": "discrete-choice.html#the-multinomial-logit-model-conceptually",
    "href": "discrete-choice.html#the-multinomial-logit-model-conceptually",
    "title": "Introduction to Discrete Choice Models",
    "section": "The multinomial logit model: conceptually",
    "text": "The multinomial logit model: conceptually\n\nEach alternative has a utility or value\nThe decisionmaker will choose the alternative with the highest utility\nLike binary logistic regression, each alternative has an error term—there is some random/unobserved variation in utility"
  },
  {
    "objectID": "discrete-choice.html#the-multinomial-logit-model-in-math-i",
    "href": "discrete-choice.html#the-multinomial-logit-model-in-math-i",
    "title": "Introduction to Discrete Choice Models",
    "section": "The multinomial logit model, in math I",
    "text": "The multinomial logit model, in math I\n\nEach alternative gets a utility function, with systematic and random components\n\n\\[\nU_{car} = V_{car} + \\epsilon_{car} \\\\\nU_{transit} = V_{transit} + \\epsilon_{transit} \\\\\nU_{bike} = V_{bike} + \\epsilon_{bike} \\\\\nU_{walk} = V_{walk} + \\epsilon_{walk} \\\\\n\\]"
  },
  {
    "objectID": "discrete-choice.html#the-multinomial-logit-model-in-math-ii",
    "href": "discrete-choice.html#the-multinomial-logit-model-in-math-ii",
    "title": "Introduction to Discrete Choice Models",
    "section": "The multinomial logit model, in math II",
    "text": "The multinomial logit model, in math II\n\nLike in a logistic regression, the systematic portion of utility is usually a linear combination of predictors\n\n\\[\nV_{car} = \\alpha_{car} + \\beta_{car,income} x_{income} \\cdots \\\\\nV_{transit} = \\alpha_{transit} + \\beta_{transit,income} x_{income} \\cdots \\\\\nV_{bike} = \\alpha_{bike} + \\beta_{bike,income} x_{income} \\cdots \\\\\nV_{walk} = \\alpha_{walk} + \\beta_{walk,income} x_{income} \\cdots \\\\\n\\]"
  },
  {
    "objectID": "discrete-choice.html#alternative-specific-constants",
    "href": "discrete-choice.html#alternative-specific-constants",
    "title": "Introduction to Discrete Choice Models",
    "section": "Alternative specific constants",
    "text": "Alternative specific constants\n\nUnlike the binary logit model, there is now an \\(\\alpha\\) for each mode\nThis is known as an alternative specific constant\nIt measures the relative popularity of each option with all other coefficients held at zero\n\nJust like in a binary logit model\n\nInterpretation is a little tricky, just like constants in other regression models—because all the other coefficients being zero doesn’t always make sense"
  },
  {
    "objectID": "discrete-choice.html#only-utility-differences-matter",
    "href": "discrete-choice.html#only-utility-differences-matter",
    "title": "Introduction to Discrete Choice Models",
    "section": "Only utility differences matter",
    "text": "Only utility differences matter\n\nThe option with the highest utility is chosen\nThis means only differences in utility matter, not their actual values\nIt is very easy to create models that are not identified (i.e. there is not a unique best set of coefficients) because of this"
  },
  {
    "objectID": "discrete-choice.html#identification-issues",
    "href": "discrete-choice.html#identification-issues",
    "title": "Introduction to Discrete Choice Models",
    "section": "Identification issues",
    "text": "Identification issues\n\nLooking again at our mode choice model, how do the differences between utilities change if we add 1 to all of the \\(\\alpha\\)’s or \\(\\beta\\)’s?\n\n\\[\nV_{car} = \\alpha_{car} + \\beta_{car,income} x_{income} \\cdots \\\\\nV_{transit} = \\alpha_{transit} + \\beta_{transit,income} x_{income} \\cdots \\\\\nV_{bike} = \\alpha_{bike} + \\beta_{bike,income} x_{income} \\cdots \\\\\nV_{walk} = \\alpha_{walk} + \\beta_{walk,income} x_{income} \\cdots \\\\\n\\]"
  },
  {
    "objectID": "discrete-choice.html#an-identified-model",
    "href": "discrete-choice.html#an-identified-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "An identified model",
    "text": "An identified model\n\nWe can solve this by setting one utility to zero\n\nWhich one does not affect predictions from the model, but will affect interpretations\n\nFor instance, we might do\n\n\\[\nV_{car} = 0 \\\\\nV_{transit} = \\alpha_{transit} + \\beta_{transit,income} x_{income} \\cdots \\\\\nV_{bike} = \\alpha_{bike} + \\beta_{bike,income} x_{income} \\cdots \\\\\nV_{walk} = \\alpha_{walk} + \\beta_{walk,income} x_{income} \\cdots \\\\\n\\]\n\nThis is exactly what a binary logistic regression does—you have coefficients for one outcome, and the utility of the other outcome is zero"
  },
  {
    "objectID": "discrete-choice.html#interpreting-the-model",
    "href": "discrete-choice.html#interpreting-the-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\nAll of the other coefficients are relative to the alternative that has its utility set to zero\nFor instance, \\(\\beta_{bike,income}\\) represents how income changes the relative attractiveness of biking and driving"
  },
  {
    "objectID": "discrete-choice.html#an-example-multinomial-logit-model-seattle-mode-choice",
    "href": "discrete-choice.html#an-example-multinomial-logit-model-seattle-mode-choice",
    "title": "Introduction to Discrete Choice Models",
    "section": "An example multinomial logit model: Seattle mode choice",
    "text": "An example multinomial logit model: Seattle mode choice\n\n\n\nWe have data on 8,883 trips taken in the Seattle region, based on a survey from the Puget Sound Regional Council\nWe have demographic information on the tripmakers, as well as attributes of the trip itself\n\nHow long it would have taken by car, bike, walk, and transit\nHow much it would have cost by car and transit\n\nFor car, based on distance traveled\n\nHow many transit transfers are required\nWhether the trip took place at rush hour"
  },
  {
    "objectID": "discrete-choice.html#interpreting-a-multinomial-logit-model",
    "href": "discrete-choice.html#interpreting-a-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting a multinomial logit model",
    "text": "Interpreting a multinomial logit model\n\nWe’re going to start with a very simple multinomial logit model, modeling the relationship between income and mode choice\n\n\n\nResults for model seattle_mode_choice\nNbr of parameters:      6\nSample size:            8883\nExcluded data:          0\nNull log likelihood:        -12314.45\nFinal log likelihood:       -6209.072\nLikelihood ratio test (null):       12210.76\nRho square (null):          0.496\nRho bar square (null):          0.495\nAkaike Information Criterion:   12430.14\nBayesian Information Criterion: 12472.7\n\n                         Name     Value  Robust std err.  Robust t-stat.  \\\n0                 asc_transit -2.510091         0.062031      -40.464799   \n1  b_income_over_100k_transit -1.465361         0.142407      -10.289965   \n2                    asc_walk -1.527040         0.040258      -37.930924   \n3     b_income_over_100k_walk  0.223509         0.055121        4.054887   \n4                    asc_bike -4.005311         0.127131      -31.505472   \n5     b_income_over_100k_bike  0.122244         0.176552        0.692396   \n\n   Robust p-value  \n0        0.000000  \n1        0.000000  \n2        0.000000  \n3        0.000050  \n4        0.000000  \n5        0.488689"
  },
  {
    "objectID": "discrete-choice.html#interpreting-a-multinomial-logit-model-alternative-specific-constants",
    "href": "discrete-choice.html#interpreting-a-multinomial-logit-model-alternative-specific-constants",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting a multinomial logit model: alternative-specific constants",
    "text": "Interpreting a multinomial logit model: alternative-specific constants\n\n\n\n\n\n\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\nasc_transit\n-2.51009\n0.0620315\n-40.4648\n0\n\n\nasc_walk\n-1.52704\n0.0402584\n-37.9309\n0\n\n\nasc_bike\n-4.00531\n0.127131\n-31.5055\n0\n\n\n\n\n\n\nFor households making less than $100,000/year\n\nBiking is least prevalent\nTransit is second least\nWalking is third least\nDriving is most prevalent"
  },
  {
    "objectID": "discrete-choice.html#interpreting-a-multinomial-logit-model-coefficients",
    "href": "discrete-choice.html#interpreting-a-multinomial-logit-model-coefficients",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting a multinomial logit model: coefficients",
    "text": "Interpreting a multinomial logit model: coefficients\n\n\n\n\n\n\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\nasc_transit\n-2.51009\n0.0620315\n-40.4648\n0\n\n\nasc_walk\n-1.52704\n0.0402584\n-37.9309\n0\n\n\nasc_bike\n-4.00531\n0.127131\n-31.5055\n0\n\n\n\n\n\n\nIn higher income households, relative to lower income households and relative to driving\n\nIs walking more prevalent than driving, or just relatively more prevalent 🤔\n\nThis is probably because this is Seattle which has an affluent, walkable downtown\n\nBiking is not really affected much by income (not statistically significant)\nTransit is relatively less prevalent\nWalking is relatively more prevalent"
  },
  {
    "objectID": "discrete-choice.html#estimating-a-multinomial-logit-model-yourself",
    "href": "discrete-choice.html#estimating-a-multinomial-logit-model-yourself",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimating a multinomial logit model yourself",
    "text": "Estimating a multinomial logit model yourself\n\n\n\nseattle_mnl_income.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\n\nV = {\n    # Car\n    1: 0,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k\n}\n\nav = {1: 1, 2: 1, 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_income\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_income\nNbr of parameters:      6\nSample size:            8883\nExcluded data:          0\nNull log likelihood:        -12314.45\nFinal log likelihood:       -6209.072\nLikelihood ratio test (null):       12210.76\nRho square (null):          0.496\nRho bar square (null):          0.495\nAkaike Information Criterion:   12430.14\nBayesian Information Criterion: 12472.7\n\n                         Name     Value  Robust std err.  Robust t-stat.  \\\n0                 asc_transit -2.510091         0.062031      -40.464799   \n1  b_income_over_100k_transit -1.465361         0.142407      -10.289965   \n2                    asc_walk -1.527040         0.040258      -37.930924   \n3     b_income_over_100k_walk  0.223509         0.055121        4.054887   \n4                    asc_bike -4.005311         0.127131      -31.505472   \n5     b_income_over_100k_bike  0.122244         0.176552        0.692396   \n\n   Robust p-value  \n0        0.000000  \n1        0.000000  \n2        0.000000  \n3        0.000050  \n4        0.000000  \n5        0.488689"
  },
  {
    "objectID": "discrete-choice.html#exercise-add-a-variable",
    "href": "discrete-choice.html#exercise-add-a-variable",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: add a variable",
    "text": "Exercise: add a variable\n\nAdd the rush_hour variable to the model - this is a binary variable for trips that occur during the AM or PM rush hours\n\nHint: you need to add it to the bike, walk, and transit utility function, with different betas"
  },
  {
    "objectID": "discrete-choice.html#exercise-add-a-variable-answer",
    "href": "discrete-choice.html#exercise-add-a-variable-answer",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: add a variable: answer",
    "text": "Exercise: add a variable: answer\n\n\n\nseattle_mnl_income_rush.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\nV = {\n    # Car\n    1: 0,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour\n}\n\nav = {1: 1, 2: 1, 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_income_rush\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_income_rush\nNbr of parameters:      9\nSample size:            8883\nExcluded data:          0\nNull log likelihood:        -12314.45\nFinal log likelihood:       -6184.064\nLikelihood ratio test (null):       12260.78\nRho square (null):          0.498\nRho bar square (null):          0.497\nAkaike Information Criterion:   12386.13\nBayesian Information Criterion: 12449.96\n\n                         Name     Value  Robust std err.  Robust t-stat.  \\\n0                 asc_transit -2.617443         0.071685      -36.513132   \n1  b_income_over_100k_transit -1.466734         0.142462      -10.295602   \n2         b_rush_hour_transit  0.420138         0.122775        3.422028   \n3                    asc_walk -1.449042         0.041688      -34.759443   \n4     b_income_over_100k_walk  0.224605         0.055248        4.065404   \n5            b_rush_hour_walk -0.424242         0.074428       -5.700046   \n6                    asc_bike -4.008908         0.134130      -29.888304   \n7     b_income_over_100k_bike  0.122184         0.176580        0.691947   \n8            b_rush_hour_bike  0.016470         0.212546        0.077487   \n\n   Robust p-value  \n0    0.000000e+00  \n1    0.000000e+00  \n2    6.215596e-04  \n3    0.000000e+00  \n4    4.794942e-05  \n5    1.197753e-08  \n6    0.000000e+00  \n7    4.889707e-01  \n8    9.382360e-01"
  },
  {
    "objectID": "discrete-choice.html#interpreting-the-coefficient-changes-i",
    "href": "discrete-choice.html#interpreting-the-coefficient-changes-i",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting the coefficient changes I",
    "text": "Interpreting the coefficient changes I\n\n\n\n\n\n\nName\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\n0\nasc_transit\n-2.61744\n0.071685\n-36.5131\n0\n\n\n1\nb_income_over_100k_transit\n-1.46673\n0.142462\n-10.2956\n0\n\n\n2\nb_rush_hour_transit\n0.420138\n0.122775\n3.42203\n0.00062156\n\n\n3\nasc_walk\n-1.44904\n0.0416877\n-34.7594\n0\n\n\n4\nb_income_over_100k_walk\n0.224605\n0.0552479\n4.0654\n4.79494e-05\n\n\n5\nb_rush_hour_walk\n-0.424242\n0.0744277\n-5.70005\n1.19775e-08\n\n\n6\nasc_bike\n-4.00891\n0.13413\n-29.8883\n0\n\n\n7\nb_income_over_100k_bike\n0.122184\n0.17658\n0.691947\n0.488971\n\n\n8\nb_rush_hour_bike\n0.0164696\n0.212546\n0.0774871\n0.938236\n\n\n\n\n\n\nBiking is about the same at rush hour vs. not\nTransit ridership is relatively higher during rush hour\nWalking is relatively lower during during rush hour"
  },
  {
    "objectID": "discrete-choice.html#interpreting-the-coefficient-changes-ii",
    "href": "discrete-choice.html#interpreting-the-coefficient-changes-ii",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting the coefficient changes II",
    "text": "Interpreting the coefficient changes II\n\n\n\n\n\n\nName\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\n0\nasc_transit\n-2.61744\n0.071685\n-36.5131\n0\n\n\n1\nb_income_over_100k_transit\n-1.46673\n0.142462\n-10.2956\n0\n\n\n2\nb_rush_hour_transit\n0.420138\n0.122775\n3.42203\n0.00062156\n\n\n3\nasc_walk\n-1.44904\n0.0416877\n-34.7594\n0\n\n\n4\nb_income_over_100k_walk\n0.224605\n0.0552479\n4.0654\n4.79494e-05\n\n\n5\nb_rush_hour_walk\n-0.424242\n0.0744277\n-5.70005\n1.19775e-08\n\n\n6\nasc_bike\n-4.00891\n0.13413\n-29.8883\n0\n\n\n7\nb_income_over_100k_bike\n0.122184\n0.17658\n0.691947\n0.488971\n\n\n8\nb_rush_hour_bike\n0.0164696\n0.212546\n0.0774871\n0.938236\n\n\n\n\n\n\nWhat happens to driving?\n\nWe have set driving as the reference level, so all of these other changes are relative to the change in driving"
  },
  {
    "objectID": "discrete-choice.html#availability",
    "href": "discrete-choice.html#availability",
    "title": "Introduction to Discrete Choice Models",
    "section": "Availability",
    "text": "Availability\n\nSometimes, not all options will be available to all decisionmakers\nFor example,\n\nIn Internet choice, some types of Internet service are not available to certain homes\nIn mode choice, transit may not be available in some areas\n\nSometimes modelers assume people without cars cannot choose driving, but a significant percentage of trips in zero-vehicle households do occur by car\n\n\nWe can add an availability constraint to our model to indicate which alternatives are available to which decisionmakers\nThis effectively sets \\(V_{mode} = -\\infty\\), so it will always be worse than every other option"
  },
  {
    "objectID": "discrete-choice.html#adding-a-transit-availability-constraint",
    "href": "discrete-choice.html#adding-a-transit-availability-constraint",
    "title": "Introduction to Discrete Choice Models",
    "section": "Adding a transit availability constraint",
    "text": "Adding a transit availability constraint\n\nWe constrain choices in the model to indicate that transit is unavailable for some trips\n\n\n\n\nseattle_mnl_availability.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\nV = {\n    # Car\n    1: 0,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour\n}\n\n# here we set the transit availability variable. We leave the other modes set to 1 - available\n# to all decisionmakers.\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_availability\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_availability\nNbr of parameters:      9\nSample size:            8883\nExcluded data:          0\nNull log likelihood:        -11985.34\nFinal log likelihood:       -inf\nLikelihood ratio test (null):       -inf\nRho square (null):          -1.8e+308\nRho bar square (null):          -1.8e+308\nAkaike Information Criterion:   inf\nBayesian Information Criterion: inf\n\n                         Name  Value  Robust std err.  Robust t-stat.  \\\n0                 asc_transit    0.0         0.056451             0.0   \n1  b_income_over_100k_transit    0.0         0.074577             0.0   \n2         b_rush_hour_transit    0.0         0.094065             0.0   \n3                    asc_walk    0.0         0.058790             0.0   \n4     b_income_over_100k_walk    0.0         0.079367             0.0   \n5            b_rush_hour_walk    0.0         0.096364             0.0   \n6                    asc_bike    0.0         0.053661             0.0   \n7     b_income_over_100k_bike    0.0         0.071824             0.0   \n8            b_rush_hour_bike    0.0         0.089161             0.0   \n\n   Robust p-value  \n0             1.0  \n1             1.0  \n2             1.0  \n3             1.0  \n4             1.0  \n5             1.0  \n6             1.0  \n7             1.0  \n8             1.0"
  },
  {
    "objectID": "discrete-choice.html#what-went-wrong",
    "href": "discrete-choice.html#what-went-wrong",
    "title": "Introduction to Discrete Choice Models",
    "section": "What went wrong?",
    "text": "What went wrong?\n\nSome people in our dataset chose transit even though (we thought) it was not available to them\nThis is fairly common\n\nSince we need attributes of all the mode options for each trip, I generated the travel_time variable with a trip planner (like Google Maps)\nI assumed that transit was only available if it was within 2.5km\nSome people might walk further than that, or drive to the bus, or bike (like me!)\nSome of the trips are also using transit systems that I did not have schedules for\n\nFor now, we’ll just remove them\nIn practice, you should look at these trips to see if they indicate errors or if removing them might bias results"
  },
  {
    "objectID": "discrete-choice.html#fixing-the-issue",
    "href": "discrete-choice.html#fixing-the-issue",
    "title": "Introduction to Discrete Choice Models",
    "section": "Fixing the issue",
    "text": "Fixing the issue\n\n\n\nseattle_mnl_availability_fixed.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\nV = {\n    # Car\n    1: 0,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour\n}\n\n# here we set the transit availability variable. We leave the other modes set to 1 - available\n# to all decisionmakers.\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_availability_fixed\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_availability_fixed\nNbr of parameters:      9\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -6102.726\nLikelihood ratio test (null):       11743.26\nRho square (null):          0.49\nRho bar square (null):          0.49\nAkaike Information Criterion:   12223.45\nBayesian Information Criterion: 12287.27\n\n                         Name     Value  Robust std err.  Robust t-stat.  \\\n0                 asc_transit -2.499443         0.072314      -34.563689   \n1  b_income_over_100k_transit -1.529168         0.147781      -10.347521   \n2         b_rush_hour_transit  0.446923         0.125502        3.561086   \n3                    asc_walk -1.449009         0.041692      -34.754849   \n4     b_income_over_100k_walk  0.224511         0.055248        4.063667   \n5            b_rush_hour_walk -0.424158         0.074426       -5.699032   \n6                    asc_bike -4.008920         0.134148      -29.884265   \n7     b_income_over_100k_bike  0.122188         0.176577        0.691980   \n8            b_rush_hour_bike  0.016515         0.212543        0.077700   \n\n   Robust p-value  \n0    0.000000e+00  \n1    0.000000e+00  \n2    3.693244e-04  \n3    0.000000e+00  \n4    4.830777e-05  \n5    1.204895e-08  \n6    0.000000e+00  \n7    4.889500e-01  \n8    9.380666e-01"
  },
  {
    "objectID": "discrete-choice.html#generic-variables",
    "href": "discrete-choice.html#generic-variables",
    "title": "Introduction to Discrete Choice Models",
    "section": "Generic variables",
    "text": "Generic variables\n\nThe variables we’ve discussed so far are “alternative-specific” — they need to have a separate coefficient for each alternative\n\nthey do not vary across alternatives for a single individual\n\n“Generic” variables vary across alternatives—for instance, travel time or cost\n\nWe can have a single coefficient across all alternatives if we want\nThis is the “generic” part—they can affect all alternatives the same way"
  },
  {
    "objectID": "discrete-choice.html#adding-a-generic-variable",
    "href": "discrete-choice.html#adding-a-generic-variable",
    "title": "Introduction to Discrete Choice Models",
    "section": "Adding a generic variable",
    "text": "Adding a generic variable\n\nLet’s add travel time to the model\nWe can use just a single coefficient\nDo we expect the coefficient to be negative or positive?"
  },
  {
    "objectID": "discrete-choice.html#adding-a-generic-variable-in-biogeme",
    "href": "discrete-choice.html#adding-a-generic-variable-in-biogeme",
    "title": "Introduction to Discrete Choice Models",
    "section": "Adding a generic variable in Biogeme",
    "text": "Adding a generic variable in Biogeme\n\n\n\nseattle_mnl_generic.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# we only need one coefficient for travel time\nb_travel_time = Beta(\"b_travel_time\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\n# but we need the travel time variables for each mode\ncar_travel_time_mins = Variable(\"car_travel_time_mins\")\nbike_travel_time_mins = Variable(\"bike_travel_time_mins\")\nwalk_travel_time_mins = Variable(\"walk_travel_time_mins\")\ntransit_travel_time_mins = Variable(\"transit_travel_time_mins\")\n\nV = {\n    # Car\n    1: b_travel_time * car_travel_time_mins,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour + b_travel_time * transit_travel_time_mins,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour + b_travel_time * walk_travel_time_mins,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour + b_travel_time * bike_travel_time_mins\n}\n\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_generic\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_generic\nNbr of parameters:      10\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -4622.165\nLikelihood ratio test (null):       14704.39\nRho square (null):          0.614\nRho bar square (null):          0.613\nAkaike Information Criterion:   9264.329\nBayesian Information Criterion: 9335.237\n\n                         Name     Value  Robust std err.  Robust t-stat.  \\\n0               b_travel_time -0.049626         0.001703      -29.139447   \n1                 asc_transit -1.170366         0.079567      -14.709189   \n2  b_income_over_100k_transit -1.462928         0.148960       -9.820971   \n3         b_rush_hour_transit  0.417214         0.130249        3.203193   \n4                    asc_walk  0.156172         0.057446        2.718571   \n5     b_income_over_100k_walk  0.384589         0.064497        5.962930   \n6            b_rush_hour_walk -0.160474         0.088025       -1.823042   \n7                    asc_bike -3.218042         0.135727      -23.709717   \n8     b_income_over_100k_bike  0.190056         0.178026        1.067575   \n9            b_rush_hour_bike  0.118855         0.214430        0.554284   \n\n   Robust p-value  \n0    0.000000e+00  \n1    0.000000e+00  \n2    0.000000e+00  \n3    1.359131e-03  \n4    6.556463e-03  \n5    2.477540e-09  \n6    6.829707e-02  \n7    0.000000e+00  \n8    2.857124e-01  \n9    5.793847e-01"
  },
  {
    "objectID": "discrete-choice.html#exercise-add-the-cost-variable",
    "href": "discrete-choice.html#exercise-add-the-cost-variable",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: add the cost variable",
    "text": "Exercise: add the cost variable\n\n\n\nseattle_mnl_generic_cost.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# we only need one coefficient for travel time\nb_travel_time = Beta(\"b_travel_time\", 0, None, None, 0)\nb_travel_cost = Beta(\"b_travel_cost\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\n# but we need the travel time variables for each mode\ncar_travel_time_mins = Variable(\"car_travel_time_mins\")\nbike_travel_time_mins = Variable(\"bike_travel_time_mins\")\nwalk_travel_time_mins = Variable(\"walk_travel_time_mins\")\ntransit_travel_time_mins = Variable(\"transit_travel_time_mins\")\n\ncar_travel_cost = Variable(\"car_travel_cost\")\nbike_travel_cost = Variable(\"bike_travel_cost\")\nwalk_travel_cost = Variable(\"walk_travel_cost\")\ntransit_travel_cost = Variable(\"transit_travel_cost\")\n\nV = {\n    # Car\n    1: b_travel_time * car_travel_time_mins + b_travel_cost * car_travel_cost,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour + b_travel_time * transit_travel_time_mins + b_travel_cost * transit_travel_cost,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour + b_travel_time * walk_travel_time_mins + b_travel_cost * walk_travel_cost,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour + b_travel_time * bike_travel_time_mins + b_travel_cost * bike_travel_cost\n}\n\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_mnl_generic_cost\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_mnl_generic_cost\nNbr of parameters:      11\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -4515.951\nLikelihood ratio test (null):       14916.81\nRho square (null):          0.623\nRho bar square (null):          0.622\nAkaike Information Criterion:   9053.902\nBayesian Information Criterion: 9131.9\n\n                          Name     Value  Robust std err.  Robust t-stat.  \\\n0                b_travel_time -0.055375         0.001872      -29.577721   \n1                b_travel_cost -0.102267         0.010528       -9.713571   \n2                  asc_transit -1.207753         0.081013      -14.908198   \n3   b_income_over_100k_transit -1.511950         0.153055       -9.878456   \n4          b_rush_hour_transit  0.145748         0.140798        1.035157   \n5                     asc_walk  0.044621         0.057456        0.776610   \n6      b_income_over_100k_walk  0.423135         0.065609        6.449382   \n7             b_rush_hour_walk -0.197586         0.088647       -2.228911   \n8                     asc_bike -3.432096         0.141653      -24.228822   \n9      b_income_over_100k_bike  0.199996         0.184738        1.082590   \n10            b_rush_hour_bike  0.058301         0.221636        0.263050   \n\n    Robust p-value  \n0     0.000000e+00  \n1     0.000000e+00  \n2     0.000000e+00  \n3     0.000000e+00  \n4     3.005957e-01  \n5     4.373887e-01  \n6     1.123073e-10  \n7     2.581982e-02  \n8     0.000000e+00  \n9     2.789903e-01  \n10    7.925123e-01"
  },
  {
    "objectID": "discrete-choice.html#willingness-to-pay",
    "href": "discrete-choice.html#willingness-to-pay",
    "title": "Introduction to Discrete Choice Models",
    "section": "Willingness to pay",
    "text": "Willingness to pay\n\nA common use of choice models is to evaluate willingness-to-pay for some attribute\nIf you have a cost coefficient, we can do this with a ratio of coefficients\nFor instance, let’s calculate value of time, in $/hour"
  },
  {
    "objectID": "discrete-choice.html#willingness-to-pay-calculation",
    "href": "discrete-choice.html#willingness-to-pay-calculation",
    "title": "Introduction to Discrete Choice Models",
    "section": "Willingness to pay: calculation",
    "text": "Willingness to pay: calculation\n\nThe units of utility are arbitrarily defined as utils\nThe units of \\(\\beta_{cost}\\) are utils per dollar\nThe units of \\(\\beta_{time}\\) are utils per minute\nThus, the units of \\(\\frac{\\beta_{time}}{\\beta_{cost}}\\) are dollars per minute"
  },
  {
    "objectID": "discrete-choice.html#willingness-to-pay-calculation-1",
    "href": "discrete-choice.html#willingness-to-pay-calculation-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Willingness to pay: calculation",
    "text": "Willingness to pay: calculation\n\\[\n\\frac{\\beta_{time}}{\\beta_{cost}} \\times 60 = \\frac{-0.055}{-0.102} \\times 60 = \\$32.35 / \\mathrm{hour}\n\\]\n\nIn theory, this is how much people are willing to spend to avoid an hour of travel time\nThis is in the ballpark of what I would expect, but is rather high\n\nSeattle is relatively affluent\nEstimated costs (esp. for driving) may be too high, because they represent total rather than marginal costs, and much of the cost of car ownership is fixed\nCosts of driving, walking, and biking are not very salient, so may be underestimated\n\nDriving costs are 0.39/mile plus $10 for parking downtown, bike costs are 0.17/mile, and walk costs are 0.10/mile\nTransit costs are loosely based on actual fares, but many transit users have monthly passes"
  },
  {
    "objectID": "discrete-choice.html#digging-more-into-the-math-of-the-multinomial-logit-model",
    "href": "discrete-choice.html#digging-more-into-the-math-of-the-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Digging more into the math of the multinomial logit model",
    "text": "Digging more into the math of the multinomial logit model"
  },
  {
    "objectID": "discrete-choice.html#predicted-probabilities-with-the-multinomial-logit-model",
    "href": "discrete-choice.html#predicted-probabilities-with-the-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Predicted probabilities with the multinomial logit model",
    "text": "Predicted probabilities with the multinomial logit model\n\nThe outcome variable of a multinomial logit model is predicted probabilities of each individual choosing each alternative\nHow do we calculate those probabilities?"
  },
  {
    "objectID": "discrete-choice.html#calculating-predicted-probabilities-with-the-multinomial-logit-model",
    "href": "discrete-choice.html#calculating-predicted-probabilities-with-the-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating predicted probabilities with the multinomial logit model",
    "text": "Calculating predicted probabilities with the multinomial logit model\nWe want to calculate the probability that decisionmaker \\(n\\) chooses alternative \\(i\\)\n\\[\nP_{ni} = P\\left[U_{ni}~~\\geq~~\\max_{\\forall j\\neq i}~~U_{nj}\\right]\n\\]\n\nWhich is probabilistic because of the error terms\n\\[\nP_{ni} = P\\left[\\left(V_{ni} + \\epsilon_{ni}\\right)~~\\geq~~\\max_{\\forall j\\neq i}~~\\left(V_{nj} + \\epsilon_{nj}\\right)\\right]\n\\]\n\n\nwhich still isn’t something we can directly calculate without accounting for (integrating over) the error terms"
  },
  {
    "objectID": "discrete-choice.html#enter-the-gumbel-distribution",
    "href": "discrete-choice.html#enter-the-gumbel-distribution",
    "title": "Introduction to Discrete Choice Models",
    "section": "Enter the Gumbel distribution",
    "text": "Enter the Gumbel distribution\n\n\n\nThe \\(\\epsilon\\)’s in the multinomial logit model are Gumbel-distributed\n\naka Type I Extreme Value\n\nThe Gumbel distribution is an approximation of the normal with attractive statistical properties\nNotably, the maximum of a set of Gumbel-distributed variables is also Gumbel-distributed\nThis gives us a closed-form solution to find \\(\\displaystyle\\max_{\\forall j\\neq i} \\left(V_{nj} + \\epsilon_{nj}\\right)\\)"
  },
  {
    "objectID": "discrete-choice.html#using-the-gumbel-distribution",
    "href": "discrete-choice.html#using-the-gumbel-distribution",
    "title": "Introduction to Discrete Choice Models",
    "section": "Using the Gumbel distribution",
    "text": "Using the Gumbel distribution\nBecause of the properties of the Gumbel distribution,\n\\[\nE[\\max_{\\forall j\\neq i} \\left(V_{nj} + \\epsilon_{nj}\\right)] = \\ln \\sum_{\\forall j\\neq i} e^{V_{nj}}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#calculating-the-choice-probability",
    "href": "discrete-choice.html#calculating-the-choice-probability",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating the choice probability",
    "text": "Calculating the choice probability\nAgain because of the properties of the Gumbel distribution, namely that the difference of two Gumbel distributions is a Logistic distribution, we have\n\\[\n\\begin{align*}\nP_{ni} =&~P\\left[\\left(V_{ni} + \\epsilon_{ni}\\right)~~\\geq~~\\max_{\\forall j\\neq i}~~\\left(V_{nj} + \\epsilon_{nj}\\right)\\right]\\\\\n=&~P\\left[\\left(V_{ni} + \\epsilon_{ni}\\right)~~\\geq~~\\ln \\sum_{\\forall j\\neq i} e^{V_{nj}}\\right]\\\\\n=&~\\frac{e^{V_{ni}}}{e^{V_{ni}} + e^{\\ln \\sum_{\\forall j\\neq i} e^{V_{nj}}}}\\\\\n=&~\\frac{e^{V_{ni}}}{\\displaystyle\\sum_{\\forall j} e^{V_{nj}}}\n\\end{align*}\n\\]\nwhich is also known as the softmax function. More detail: see Section 5.2 of Ben-Akiva & Lerman (1985) Discrete Choice Analysis: Theory and Application to Travel Demand"
  },
  {
    "objectID": "discrete-choice.html#the-expected-maximum-utilitylogsum",
    "href": "discrete-choice.html#the-expected-maximum-utilitylogsum",
    "title": "Introduction to Discrete Choice Models",
    "section": "The expected maximum utility/logsum",
    "text": "The expected maximum utility/logsum\n\nThe denominator of the probability function is the “expected maximum utility” (in exponentiated form), also called the “logsum”\nThis estimates how much utility a decisionmaker will derive from their chosen outcome, without having to know what they choose\nFor instance, in a mode choice model, this might be used as an accessibility metric, a measure of the destinations accessible via the transportation system\nConceptually similar to a weighted sum used for evaluation,\n\nbut weights derived analytically from stated or revealed preferences"
  },
  {
    "objectID": "discrete-choice.html#only-utility-differences-matter-1",
    "href": "discrete-choice.html#only-utility-differences-matter-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Only utility differences matter",
    "text": "Only utility differences matter\nWhat happens if we add 1.5 to all utilities?\n\\[\n\\begin{align*}\nP_{ni} &= \\frac{e^{V_{ni} + 1.5}}{\\displaystyle\\sum_{\\forall j} e^{V_{nj} + 1.5}} \\\\\n&= \\frac{e^{V_{ni}}e^{1.5}}{\\displaystyle\\sum_{\\forall j} e^{V_{nj}}e^{1.5}} \\\\\n&= \\frac{e^{V_{ni}}e^{1.5}}{e^{1.5}\\displaystyle\\sum_{\\forall j} e^{V_{nj}}} \\\\\n&= \\frac{e^{V_{ni}}}{\\displaystyle\\sum_{\\forall j} e^{V_{nj}}} \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#the-binary-and-multinomial-logit-are-the-same-model",
    "href": "discrete-choice.html#the-binary-and-multinomial-logit-are-the-same-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "The binary and multinomial logit are the same model",
    "text": "The binary and multinomial logit are the same model\n\nThe binary logit model is just a multinomial logit model with two outcome and no generic variables\nIf we had two outcomes, 0 and 1, with systematic utilities \\(V_0\\) and \\(V_1\\), the choice probability would be, with \\(V_0 = 0\\)\n\n\\[\n\\begin{align}\nP(y = 1) &= \\frac{e^{V_1}}{e^{V_0} + e^{V_1}}\\\\\n&= \\frac{e^{V_1}}{e^0 + e^{V_1}}\\\\\n&= \\frac{e^{V_1}}{1 + e^{V_1}}\n\\end{align}\n\\]\nWhich is the same as the predicted probability in binary logistic regression"
  },
  {
    "objectID": "discrete-choice.html#calculating-choice-probabilities-in-biogeme",
    "href": "discrete-choice.html#calculating-choice-probabilities-in-biogeme",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating choice probabilities in Biogeme",
    "text": "Calculating choice probabilities in Biogeme\nAdd to the end of your most recent model, and run\n\n# models.loglogit(V, av, Variable(\"mode_choice\")) represents the log-probability of the chosen alternative\n# models.logit(V, av, 1) represents the probability (not logged) of alternative 1\nsim_model = bio.BIOGEME(data, {\n    \"Car\": models.logit(V, av, 1),\n    \"Transit\": models.logit(V, av, 2),\n    \"Walk\": models.logit(V, av, 3),\n    \"Bike\": models.logit(V, av, 4)\n})\n\nchoice_probabilities = sim_model.simulate(result.get_beta_values())\nprint(choice_probabilities)\n\n           Car   Transit          Walk      Bike\n0     0.998260  0.000000  4.856248e-06  0.001736\n1     0.998383  0.000000  4.638507e-06  0.001613\n2     0.976717  0.000000  9.568361e-03  0.013715\n3     0.980814  0.007712  9.379111e-04  0.010536\n4     0.618460  0.020160  3.409841e-01  0.020396\n...        ...       ...           ...       ...\n8868  0.999776  0.000000  7.369275e-10  0.000224\n8869  0.998289  0.000000  1.323505e-06  0.001710\n8870  0.999972  0.000000  1.219416e-13  0.000028\n8871  0.999776  0.000000  7.369275e-10  0.000224\n8872  0.850049  0.144213  1.193309e-05  0.005726\n\n[8873 rows x 4 columns]"
  },
  {
    "objectID": "discrete-choice.html#calculating-market-shares",
    "href": "discrete-choice.html#calculating-market-shares",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating market shares",
    "text": "Calculating market shares\n\nMarket shares are just the mean probability across all individuals\n\n\nprint(choice_probabilities.apply(np.mean))\n\nCar        0.761974\nTransit    0.037530\nWalk       0.185732\nBike       0.014765\ndtype: float64\n\n\nThis is exactly the same as the market shares in the original data. This will always be the case when we have ASCs.\n\ndf.mode_choice.value_counts() / len(df)\n\nmode_choice\nCar        0.761975\nWalk       0.185732\nTransit    0.037530\nBike       0.014764\nName: count, dtype: float64"
  },
  {
    "objectID": "discrete-choice.html#simulation",
    "href": "discrete-choice.html#simulation",
    "title": "Introduction to Discrete Choice Models",
    "section": "Simulation",
    "text": "Simulation\n\nWashington State is considering a $0.025/mile road usage tax\nLet’s model what this might do to market shares\nWe’ll increase the cost of driving by $0.025/mile\n\nThe actual proposal would also get rid of gas taxes, but that is complicated to model because it depends on the fuel economy of the vehicle"
  },
  {
    "objectID": "discrete-choice.html#simulation-1",
    "href": "discrete-choice.html#simulation-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Simulation",
    "text": "Simulation\n\ndf_road_cost = df.copy() # create a copy of the data to modify\ndf_road_cost[\"car_travel_cost\"] += df_road_cost.car_distance_km * 0.025 / 1.609\ndata_road_cost = db.Database(\"Seattle road cost\", pd.get_dummies(df_road_cost).astype(\"float64\"))\n\nroad_cost_model = bio.BIOGEME(data_road_cost, {\n    \"Car\": models.logit(V, av, 1),\n    \"Transit\": models.logit(V, av, 2),\n    \"Walk\": models.logit(V, av, 3),\n    \"Bike\": models.logit(V, av, 4)\n})\n\nchoice_probabilities = road_cost_model.simulate(result.get_beta_values())\nprint(choice_probabilities.apply(np.mean))\n\nCar        0.760968\nTransit    0.037977\nWalk       0.186166\nBike       0.014889\ndtype: float64"
  },
  {
    "objectID": "discrete-choice.html#how-much-do-we-forecast-driving-will-decrease",
    "href": "discrete-choice.html#how-much-do-we-forecast-driving-will-decrease",
    "title": "Introduction to Discrete Choice Models",
    "section": "How much do we forecast driving will decrease?",
    "text": "How much do we forecast driving will decrease?\n\n76.2% -&gt; 76.1%"
  },
  {
    "objectID": "discrete-choice.html#exercise-make-transit-faster",
    "href": "discrete-choice.html#exercise-make-transit-faster",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: make transit faster",
    "text": "Exercise: make transit faster\n\nImprove transit travel times by 25% and see how choice probabilities change\nDon’t keep the road cost scenario\nHint: df_transit_time[\"transit_travel_time_mins\"] *= 0.75 scales the variable transit_travel_time by 75%"
  },
  {
    "objectID": "discrete-choice.html#exercise-answers",
    "href": "discrete-choice.html#exercise-answers",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise answers",
    "text": "Exercise answers\n\ndf_transit_time = df.copy() # create a copy of the data to modify\ndf_transit_time[\"transit_travel_time_mins\"] *= 0.75\ndata_transit_time = db.Database(\"Seattle transit time\", pd.get_dummies(df_transit_time).astype(\"float64\"))\n\ntransit_time_model = bio.BIOGEME(data_transit_time, {\n    \"Car\": models.logit(V, av, 1),\n    \"Transit\": models.logit(V, av, 2),\n    \"Walk\": models.logit(V, av, 3),\n    \"Bike\": models.logit(V, av, 4)\n})\n\nchoice_probabilities = transit_time_model.simulate(result.get_beta_values())\nprint(choice_probabilities.apply(np.mean))\n\nCar        0.741874\nTransit    0.059873\nWalk       0.183807\nBike       0.014447\ndtype: float64"
  },
  {
    "objectID": "discrete-choice.html#result-1",
    "href": "discrete-choice.html#result-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Result",
    "text": "Result\n\nTransit mode share increase from 3.8% to 6.0%"
  },
  {
    "objectID": "discrete-choice.html#partial-and-general-equilibrium-results",
    "href": "discrete-choice.html#partial-and-general-equilibrium-results",
    "title": "Introduction to Discrete Choice Models",
    "section": "Partial and general equilibrium results",
    "text": "Partial and general equilibrium results\n\nThe results above are partial equilibrium or marginal results\nThey represent forecasted changes if everything else stays the same\nThey do not account for general equilibrium changes, which is how the rest of the market adjusts in response to a change\n\nFor instance, if road pricing or better transit decrease auto usage, we might see auto travel times drop and see auto usage somewhat rebound"
  },
  {
    "objectID": "discrete-choice.html#how-the-coefficients-are-estimated",
    "href": "discrete-choice.html#how-the-coefficients-are-estimated",
    "title": "Introduction to Discrete Choice Models",
    "section": "How the coefficients are estimated",
    "text": "How the coefficients are estimated\n\nA multinomial logit model is estimated using maximum likelihood\nWe want to find the set of coefficients that is most likely given the data we have observed\nWe then turn this idea on its head, and find the set of coefficients that makes the data we observed most likely\nWe do this by defining a likelihood function, which is the probability of observing the data we observed given a set of coefficients\nWe then use an iterative process to find the set of coefficients that maximizes this probability"
  },
  {
    "objectID": "discrete-choice.html#the-likelihood-function-for-discrete-choice-models",
    "href": "discrete-choice.html#the-likelihood-function-for-discrete-choice-models",
    "title": "Introduction to Discrete Choice Models",
    "section": "The likelihood function for discrete choice models",
    "text": "The likelihood function for discrete choice models\n\nThe likelihood function for a discrete choice model is just the product of the choice probabilities of the chosen outcome for each decisionmaker\n\n\\[\n\\begin{align*}\nL =&~\\prod_n P_{n,i=\\mathrm{chosen}(n)}\\\\\n  =&~\\prod_n \\frac{e^{V_{n,i=\\mathrm{chosen}(n)}}}{\\sum_j e^{V_{nj}}}\n\\end{align*}\n\\]\n\n\nIs this likely to be a large number?"
  },
  {
    "objectID": "discrete-choice.html#the-log-likelihood",
    "href": "discrete-choice.html#the-log-likelihood",
    "title": "Introduction to Discrete Choice Models",
    "section": "The log-likelihood",
    "text": "The log-likelihood\n\nThe likelihood is too small to comptue with\n\nThe smallest non-zero number than can be represented varies by machine, but is in the neighborhood of \\(5 \\times 10^{-324}\\)\nThe likelihood for the model we just estimated is about \\(10^{-1961}\\)\nYou can confirm this if you want by running 10 ** -1961 in Python, you will get 0\n\nInstead, we compute with the loglikelihood - i.e. the natural logarithm of the likelihood"
  },
  {
    "objectID": "discrete-choice.html#a-few-things-to-remember-about-logs",
    "href": "discrete-choice.html#a-few-things-to-remember-about-logs",
    "title": "Introduction to Discrete Choice Models",
    "section": "A few things to remember about logs",
    "text": "A few things to remember about logs\n\n\n\nLogs map numbers between 0 and 1 to \\(-\\infty\\) to 0\nVery small probabilities are very large negative numbers, avoiding rounding problems\n\\(\\ln ab = \\ln a + \\ln b\\)"
  },
  {
    "objectID": "discrete-choice.html#the-log-likelihood-function-for-a-multinomial-logit-model",
    "href": "discrete-choice.html#the-log-likelihood-function-for-a-multinomial-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "The log-likelihood function for a multinomial logit model",
    "text": "The log-likelihood function for a multinomial logit model\n\\[\n\\ell = \\sum_{n} \\ln \\frac{e^{V_{n,i=\\mathrm{chosen}(n)}}}{\\sum_j e^{V_{nj}}}\n\\]\n\nWe take the log of each probability\nThe product becomes a summation\nWe see the log-likelihood in the Biogeme output\nWhy was the log-likelihood \\(-\\infty\\) when we first added the availability constraint?\n\nBecause some people chose a mode that was not available to them, making the probability 0"
  },
  {
    "objectID": "discrete-choice.html#pseudo-r2-statistics",
    "href": "discrete-choice.html#pseudo-r2-statistics",
    "title": "Introduction to Discrete Choice Models",
    "section": "Pseudo-\\(R^2\\) statistics",
    "text": "Pseudo-\\(R^2\\) statistics\n\nIn linear regression, we have the \\(R^2\\) statistic that tells us how well the model fits the data\nWe don’t have anything quite the same for discrete choice models\nMany authors have created pseudo-\\(R^2\\) (sometimes called \\(\\rho^2\\)) metrics based on loglikelihoods\nThese measure the loglikelihood of your model relative to a null model, either\n\na model with no coefficients, predicting equal probabilities for all alternatives, or\na model with only ASCs (constants), predicting base rates for all alternatives"
  },
  {
    "objectID": "discrete-choice.html#mcfaddens-pseudo-r2",
    "href": "discrete-choice.html#mcfaddens-pseudo-r2",
    "title": "Introduction to Discrete Choice Models",
    "section": "McFadden’s pseudo-\\(R^2\\)",
    "text": "McFadden’s pseudo-\\(R^2\\)\n\nMost common is probably McFadden’s, \\(1 - \\frac{\\ell}{\\ell_C}\\)\nOr McFadden’s adjusted, \\(1 - \\frac{\\ell - K}{\\ell_C}\\) where \\(K\\) is number of estimated coefficients that weren’t in the null model, which attempts to correct for overfitting\nAnecdotally, pseudo-\\(R^2\\) statistics tend to be fairly low"
  },
  {
    "objectID": "discrete-choice.html#calculating-pseudo-r2-in-biogeme",
    "href": "discrete-choice.html#calculating-pseudo-r2-in-biogeme",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating pseudo-\\(R^2\\) in Biogeme",
    "text": "Calculating pseudo-\\(R^2\\) in Biogeme\n\nBiogeme automatically calculates the log-likelihood at zero\nHowever, comparing to log-likelihood at constants is generally preferred\n\nThis is more similar to linear regression, where the \\(R^2\\) is relative to a model with only an intercept\n\nUnfortunately, Biogeme does not calculate this automatically\nIt is pretty easy to calculate though, by removing everything but the ASCs\nExercise: go ahead and do this now"
  },
  {
    "objectID": "discrete-choice.html#calculating-pseudo-r2-in-biogeme-results",
    "href": "discrete-choice.html#calculating-pseudo-r2-in-biogeme-results",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating pseudo-\\(R^2\\) in Biogeme: results",
    "text": "Calculating pseudo-\\(R^2\\) in Biogeme: results\n\n\n\nseattle_constants.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\n\n# we only need one coefficient for travel time\nb_travel_time = Beta(\"b_travel_time\", 0, None, None, 0)\nb_travel_cost = Beta(\"b_travel_cost\", 0, None, None, 0)\n\n\nV = {\n    # Car\n    1: 0,\n\n    # Transit\n    2: asc_transit,\n\n    # Walk\n    3: asc_walk,\n\n    # Bike\n    4: asc_bike \n}\n\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\nlogprob = models.loglogit(V, av, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_constants\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_constants\nNbr of parameters:      3\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -6211.13\nLikelihood ratio test (null):       11526.46\nRho square (null):          0.481\nRho bar square (null):          0.481\nAkaike Information Criterion:   12428.26\nBayesian Information Criterion: 12449.53\n\n          Name     Value  Robust std err.  Robust t-stat.  Robust p-value\n0  asc_transit -2.868313         0.056243      -50.998228             0.0\n1     asc_walk -1.411608         0.027472      -51.383789             0.0\n2     asc_bike -3.943729         0.088213      -44.706999             0.0"
  },
  {
    "objectID": "discrete-choice.html#calculating-pseudo-r2-in-biogeme-1",
    "href": "discrete-choice.html#calculating-pseudo-r2-in-biogeme-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Calculating pseudo-\\(R^2\\) in Biogeme",
    "text": "Calculating pseudo-\\(R^2\\) in Biogeme\n\nLog-likelihood at constants: -6211.13\nLog-likelihood at convergence: -4515.951\nMcFadden’s pseudo-\\(R^2\\): \\(1 - \\frac{-4515.951}{-6211.13} = 0.273\\)\nEstimated parameters other than ASCs: 8\nMcFadden’s adjusted pseudo-\\(R^2\\): \\(1 - \\frac{-4515.951 - 8}{-6211.13} = 0.272\\)"
  },
  {
    "objectID": "discrete-choice.html#comparing-coefficients-of-logistic-regression-models",
    "href": "discrete-choice.html#comparing-coefficients-of-logistic-regression-models",
    "title": "Introduction to Discrete Choice Models",
    "section": "Comparing coefficients of logistic regression models",
    "text": "Comparing coefficients of logistic regression models\n\nIn general, you can’t directly compare coefficients from different logistic regressions\nThey may be scaled differently\nIn a linear regression, the coefficients have the same scale as the dependent variable\nBut in logistic regression, utility is scaleless"
  },
  {
    "objectID": "discrete-choice.html#comparing-coefficients-of-logistic-regression-models-1",
    "href": "discrete-choice.html#comparing-coefficients-of-logistic-regression-models-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "Comparing coefficients of logistic regression models",
    "text": "Comparing coefficients of logistic regression models\n\n\n\nTrue model: \\(V_1 = 0.3 + 2x_1 + 2x_2 + 2x_3\\)\n\\(x_1\\), \\(x_2\\), \\(x_3\\) uncorrelated\n\n\n\n\n\n\nVariable\n\n\nModel 1\n\n\nModel 2\n\n\nModel 3\n\n\n\n\n\n\n(Intercept)\n\n\n0.166***\n\n\n0.209***\n\n\n0.334***\n\n\n\n\nx1\n\n\n0.984***\n\n\n1.273***\n\n\n1.995***\n\n\n\n\nx2\n\n\n-\n\n\n1.288***\n\n\n2.002***\n\n\n\n\nx3\n\n\n-\n\n\n-\n\n\n1.95***"
  },
  {
    "objectID": "discrete-choice.html#data-for-choice-modeling-stated-preference-data",
    "href": "discrete-choice.html#data-for-choice-modeling-stated-preference-data",
    "title": "Introduction to Discrete Choice Models",
    "section": "Data for choice modeling: stated preference data",
    "text": "Data for choice modeling: stated preference data\n\nThere are two broad types of data for choice models: stated preference and revealed preference\nStated preference data comes from surveys where you ask people to choose between options\nThat might look something like this:\n\nOf the following options for internet service, which would you select:\n\nCable modem, 100 megabits/second, 1 outage per month, $55 / month, $50 setup fee\nFiber, 1000 megabits/second, 2 outages per year, $100 / month, $95 setup fee\nDSL, 6 megabits/second, 2 outages / month, $30 / month, $25 setup fee"
  },
  {
    "objectID": "discrete-choice.html#modeling-with-choice-experiments",
    "href": "discrete-choice.html#modeling-with-choice-experiments",
    "title": "Introduction to Discrete Choice Models",
    "section": "Modeling with choice experiments",
    "text": "Modeling with choice experiments\n\nYou then run a choice model on the outcomes, using the attributes of the different alternatives as independent variables\nYou need to make sure the variables aren’t perfectly correlated\n\nfor instance, in the above example, if the prices always stayed the same, the model wouldn’t be able to differentiate setup fee from monthly fee preferences\nif you have ASCs in your model, this is even more of an issue—if fiber is always 1000 megabits/second, you won’t be able to differentiate the speed from an inherent preference for fiber\n\nThere’s a lot more that goes into designing good choice experiments; see Bliemer & Rose (2024) Designing and conducting stated choice experiments Handbook of Choice Modelling"
  },
  {
    "objectID": "discrete-choice.html#revealed-preference-data-attributes-of-alternatives",
    "href": "discrete-choice.html#revealed-preference-data-attributes-of-alternatives",
    "title": "Introduction to Discrete Choice Models",
    "section": "Revealed preference data: attributes of alternatives",
    "text": "Revealed preference data: attributes of alternatives\n\nA key challenge with revealed preference data is that you don’t observe the attributes of the alternatives, and have to estimate them somehow\nHow you do this depends on the choice, of course\nIn transportation, using trip-planning software is common for attributes of alternative routes\nIn residential location choice, it is common to just randomly sample a few alternatives\nMultinomial logit parameters are consistently estimated when alternatives are sampled randomly (ch. 8, Ben-Akiva & Lerman (1985) Discrete Choice Analysis: Theory and Application to Travel Demand)"
  },
  {
    "objectID": "discrete-choice.html#wide-and-long-data",
    "href": "discrete-choice.html#wide-and-long-data",
    "title": "Introduction to Discrete Choice Models",
    "section": "Wide and long data",
    "text": "Wide and long data\n\nAll of the data we have used so far has been “wide”—there is one row per decision, and attributes of alternatives are in columns (e.g. car_travel_time, walk_travel_time, etc.)\n\n\n\n\nID\nChosen\ncar_travel_time\nwalk_travel_time\n\n\n\n\n1\nCar\n10\n23\n\n\n\n\nYou may sometimes find “long” data—with multiple rows per choice with attributes of different alternatives\n\n\n\n\nID\nChoice\ntravel_time\nChosen\n\n\n\n\n1\nCar\n10\nYes\n\n\n1\nWalk\n23\nNo\n\n\n\n\nBiogeme requires wide data; pandas provides the unstack method to convert between formats"
  },
  {
    "objectID": "discrete-choice.html#independence-of-irrelevant-alternatives",
    "href": "discrete-choice.html#independence-of-irrelevant-alternatives",
    "title": "Introduction to Discrete Choice Models",
    "section": "Independence of irrelevant alternatives",
    "text": "Independence of irrelevant alternatives\n\nThe multinomial logit model assumes the \\(\\epsilon\\)’s are independent\nThat is, there are no shared unobserved attributes of alternatives (i.e. not in the model)\nA significant implication of this assumption is the independence of irrelevant alternatives property\nThis says that when the probability of choosing one alternative increases (decreases), the probability of choosing the others decreases (increases) proportionally to the original probability of choosing that mode\n\nTrue at the individual level, not generally true of total forecast market shares"
  },
  {
    "objectID": "discrete-choice.html#the-red-busblue-bus-problem",
    "href": "discrete-choice.html#the-red-busblue-bus-problem",
    "title": "Introduction to Discrete Choice Models",
    "section": "The red bus/blue bus problem",
    "text": "The red bus/blue bus problem\n\nSuppose you have a model of mode choice, with two alternatives: driving and a red bus\nOne individual has a 50% probability of choosing each alternative\nSuppose you paint half the buses red but don’t change anything about the service"
  },
  {
    "objectID": "discrete-choice.html#the-red-busblue-bus-problem-1",
    "href": "discrete-choice.html#the-red-busblue-bus-problem-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "The red bus/blue bus problem",
    "text": "The red bus/blue bus problem\n\nYou’d expect this individual to now have probabilities 25% red bus, 25% blue bus, and 50% drive\nBut, half of the probability has to come from each other alternative\nSo, if blue bus has 25% probability, existing choices must all reduce by the same factor\nRed bus: 50% * .75 = 37.5%\nSame for car"
  },
  {
    "objectID": "discrete-choice.html#the-nested-logit-model",
    "href": "discrete-choice.html#the-nested-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "The nested logit model",
    "text": "The nested logit model\n\nThe nested logit model relaxes the independence of irrelevant alternatives assumption by allowing error correlation between pre-specified alternatives (known as “nests”)\nMathematically, it represents a hierarchical choice where first you choose among nests and then among alternatives within nests"
  },
  {
    "objectID": "discrete-choice.html#the-nested-logit-model-1",
    "href": "discrete-choice.html#the-nested-logit-model-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "The nested logit model",
    "text": "The nested logit model\n\nFor instance, we might think that transit and driving share unobserved attributes\n\nNamely, they are both motorized so are more suited for traveling long distances\nThey are also more protected from rainy Seattle weather\n\nWe will estimate this nested logit model\n\n\n\n\n\n\nflowchart TD\n    mc[\"Mode choice\"]\n    mc --&gt; Motorized\n    mc --&gt; Walk\n    mc --&gt; Bike\n    Motorized --&gt; Car\n    Motorized --&gt; Transit"
  },
  {
    "objectID": "discrete-choice.html#the-nested-logit-model-mathematically",
    "href": "discrete-choice.html#the-nested-logit-model-mathematically",
    "title": "Introduction to Discrete Choice Models",
    "section": "The nested logit model: mathematically",
    "text": "The nested logit model: mathematically\n\nThe nested logit model is mathematically a hierarchical model—first, a choice between the top-level alternatives\n\ne.g. Motorized, Walk, or Bike\n\nThen, within each nest, a choice of alternatives within that nest\nAt the top level, the utility of a nest is based on the utility of the items within that nest\nThe probability of choosing an alternative within a nest is the probability of choosing that nest multiplied by the probability of choosing the alternative given that the nest is chosen, e.g.\n\n\\[\nP(car) = P(motorized) \\times P(car|motorized)\n\\]\n\nSome people actually implement the nested logit model by estimating separate multinomial logit models for each nest and between the nests\n\nNot common anymore, modern computers and software can estimate all levels simultaneously which is statistically preferable"
  },
  {
    "objectID": "discrete-choice.html#expected-maximum-utility",
    "href": "discrete-choice.html#expected-maximum-utility",
    "title": "Introduction to Discrete Choice Models",
    "section": "Expected maximum utility",
    "text": "Expected maximum utility\n\nThe utility of a nest is the maximum of the utilities of the members of that nest\nWe’ve already seen the formula for this, thanks to our Gumbel-distributed errors:\n\n\\[\n\\Gamma_{\\mathrm{nest}} = \\ln \\sum_{j \\in \\mathrm{nest}} e^V_{j}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#top-level-choice-probability",
    "href": "discrete-choice.html#top-level-choice-probability",
    "title": "Introduction to Discrete Choice Models",
    "section": "Top-level choice probability",
    "text": "Top-level choice probability\n\nThe top-level choice probability is like a multinomial logit, with the expected maximum utility for the nest\n\n\\[\nP(motorized) = \\frac{e^{\\ln \\sum_{j \\in \\mathrm{motorized}} e^{V_{j}}}}{e^{\\ln \\sum_{j \\in \\mathrm{motorized}} V_{j}} + e^{V_{bike}} + e^{V_{walk}}}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#this-reduces-to-the-multinomial-logit",
    "href": "discrete-choice.html#this-reduces-to-the-multinomial-logit",
    "title": "Introduction to Discrete Choice Models",
    "section": "This reduces to the multinomial logit",
    "text": "This reduces to the multinomial logit\n\nWith some math we can show this is equivalent to the multinomial logit model\n\n\\[\n\\begin{align*}\nP(motorized) &= \\frac{e^{\\ln \\sum_{j \\in \\mathrm{motorized}} e^{V_{j}}}}{e^{\\ln \\sum_{j \\in \\mathrm{motorized}} e^{V_j}} + e^{V_{bike}} + e^{V_{walk}}} \\\\\n             &= \\frac{\\sum_{j \\in \\mathrm{motorized}} e^{V_j}}{\\sum_{j \\in \\mathrm{motorized}} e^{V_j} + e^{V_{bike}} + e^{V_{walk}}} \\\\\n             &= \\frac{e^{V_{car}} + e^{V_{transit}}}{e^{V_{car}} + e^{V_{transit}} + e^{V_{bike}} + e^{V_{walk}}} \\\\\n             &= \\frac{e^{V_{car}}}{e^{V_{car}} + e^{V_{transit}} + e^{V_{bike}} + e^{V_{walk}}} + \\frac{e^{V_{transit}}}{e^{V_{car}} + e^{V_{bike}} + e^{V_{bike}} + e^{V_{walk}}} \\\\\n             &= P_{mnl}(car) + P_{mnl}(transit)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#scale-parameters",
    "href": "discrete-choice.html#scale-parameters",
    "title": "Introduction to Discrete Choice Models",
    "section": "Scale parameters",
    "text": "Scale parameters\n\nIf the nested logit is equivalent to the multinomial logit, why bother?\nThere is an additional term that is introduced in the nested logit, the inclusive value parameter \\(\\theta\\)\nThe utility of choosing a nest, then, is\n\n\\[\n\\theta \\ln \\sum_{j \\in nest} e^{V_j / \\theta}\n\\]"
  },
  {
    "objectID": "discrete-choice.html#what-does-the-scaling-parameter-do",
    "href": "discrete-choice.html#what-does-the-scaling-parameter-do",
    "title": "Introduction to Discrete Choice Models",
    "section": "What does the scaling parameter do?",
    "text": "What does the scaling parameter do?\n\\[\n\\theta \\ln \\sum_{j \\in nest} e^{V_j / \\theta}\n\\]\n\nThe first \\(\\theta\\) scales the expected maximum utility to account for correlation between the error terms of the items in the nest\nWhen the error terms are uncorrelated, it is fairly likely that one of them will result in relatively high utility, just by chance\n\nBecause there are many chances for an error term to be large\n\nWhen they are correlated, it is less likely\n\nAt the extreme, if the error terms were perfectly correlated, there would only be one chance for the error term to result in a large utility\n\nThe second \\(\\theta\\) normalizes the utilities so that the coefficients are comparable across alternatives in different nests"
  },
  {
    "objectID": "discrete-choice.html#the-range-of-theta",
    "href": "discrete-choice.html#the-range-of-theta",
    "title": "Introduction to Discrete Choice Models",
    "section": "The range of \\(\\theta\\)",
    "text": "The range of \\(\\theta\\)\n\n\\(\\theta\\) should be between 0 and 1\nIf it is not, the model is not consistent with random utility theory"
  },
  {
    "objectID": "discrete-choice.html#estimating-a-nested-logit-model-in-biogeme",
    "href": "discrete-choice.html#estimating-a-nested-logit-model-in-biogeme",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimating a nested logit model in Biogeme",
    "text": "Estimating a nested logit model in Biogeme\n\nWe will put transit and driving together in a “motorized” nest.\nBiogeme does not estimated \\(\\theta\\), instead it estimates \\(\\mu\\) which is \\(\\frac{1}{\\theta}\\)"
  },
  {
    "objectID": "discrete-choice.html#estimating-a-nested-logit-model-in-biogeme-code",
    "href": "discrete-choice.html#estimating-a-nested-logit-model-in-biogeme-code",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimating a nested logit model in Biogeme: code",
    "text": "Estimating a nested logit model in Biogeme: code\n\n\n\nseattle_nested.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.nests import OneNestForNestedLogit, NestsForNestedLogit\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# we only need one coefficient for travel time\nb_travel_time = Beta(\"b_travel_time\", 0, None, None, 0)\nb_travel_cost = Beta(\"b_travel_cost\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\n# but we need the travel time variables for each mode\ncar_travel_time_mins = Variable(\"car_travel_time_mins\")\nbike_travel_time_mins = Variable(\"bike_travel_time_mins\")\nwalk_travel_time_mins = Variable(\"walk_travel_time_mins\")\ntransit_travel_time_mins = Variable(\"transit_travel_time_mins\")\n\ncar_travel_cost = Variable(\"car_travel_cost\")\nbike_travel_cost = Variable(\"bike_travel_cost\")\nwalk_travel_cost = Variable(\"walk_travel_cost\")\ntransit_travel_cost = Variable(\"transit_travel_cost\")\n\nV = {\n    # Car\n    1: b_travel_time * car_travel_time_mins + b_travel_cost * car_travel_cost,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour + b_travel_time * transit_travel_time_mins + b_travel_cost * transit_travel_cost,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour + b_travel_time * walk_travel_time_mins + b_travel_cost * walk_travel_cost,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour + b_travel_time * bike_travel_time_mins + b_travel_cost * bike_travel_cost\n}\n\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\n# specify nests\nmu_motorized = Beta(\"mu_motorized\", 1, None, None, 0)\nmotorized = OneNestForNestedLogit(nest_param=mu_motorized, list_of_alternatives=[1, 2], name=\"motorized\")\n\n# If you had multiple nests, you would add them to tuple_of_nests after the comma.\n# there always has to be a comma, even when there is only one nest\nnests = NestsForNestedLogit(choice_set=V.keys(), tuple_of_nests=(motorized,))\n\nlogprob = models.lognested(V, av, nests, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_nested\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_nested\nNbr of parameters:      12\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -4509.657\nLikelihood ratio test (null):       14929.4\nRho square (null):          0.623\nRho bar square (null):          0.622\nAkaike Information Criterion:   9043.314\nBayesian Information Criterion: 9128.403\n\n                          Name     Value  Robust std err.  Robust t-stat.  \\\n0                b_travel_time -0.057707         0.002075      -27.806855   \n1                b_travel_cost -0.109088         0.013781       -7.916038   \n2                 mu_motorized  0.758906         0.080905        9.380201   \n3                  asc_transit -1.900959         0.332973       -5.709042   \n4   b_income_over_100k_transit -1.964144         0.268476       -7.315903   \n5          b_rush_hour_transit  0.268330         0.186491        1.438841   \n6                     asc_walk  0.093255         0.061557        1.514949   \n7      b_income_over_100k_walk  0.412714         0.067211        6.140570   \n8             b_rush_hour_walk -0.191103         0.090473       -2.112273   \n9                     asc_bike -3.404355         0.144008      -23.640075   \n10     b_income_over_100k_bike  0.188942         0.185916        1.016276   \n11            b_rush_hour_bike  0.062802         0.223188        0.281386   \n\n    Robust p-value  \n0     0.000000e+00  \n1     2.442491e-15  \n2     0.000000e+00  \n3     1.136138e-08  \n4     2.555733e-13  \n5     1.501956e-01  \n6     1.297854e-01  \n7     8.222605e-10  \n8     3.466300e-02  \n9     0.000000e+00  \n10    3.094982e-01  \n11    7.784143e-01"
  },
  {
    "objectID": "discrete-choice.html#nested-logit-results",
    "href": "discrete-choice.html#nested-logit-results",
    "title": "Introduction to Discrete Choice Models",
    "section": "Nested logit results",
    "text": "Nested logit results\n\n\n\n\n\n\nName\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\n0\nb_travel_time\n-0.0577069\n0.00207528\n-27.8069\n0\n\n\n1\nb_travel_cost\n-0.109088\n0.0137806\n-7.91604\n2.44249e-15\n\n\n2\nmu_motorized\n0.758906\n0.0809051\n9.3802\n0\n\n\n3\nasc_transit\n-1.90096\n0.332973\n-5.70904\n1.13614e-08\n\n\n4\nb_income_over_100k_transit\n-1.96414\n0.268476\n-7.3159\n2.55573e-13\n\n\n5\nb_rush_hour_transit\n0.26833\n0.186491\n1.43884\n0.150196\n\n\n6\nasc_walk\n0.0932552\n0.0615567\n1.51495\n0.129785\n\n\n7\nb_income_over_100k_walk\n0.412714\n0.0672111\n6.14057\n8.2226e-10\n\n\n8\nb_rush_hour_walk\n-0.191103\n0.0904728\n-2.11227\n0.034663\n\n\n9\nasc_bike\n-3.40435\n0.144008\n-23.6401\n0\n\n\n10\nb_income_over_100k_bike\n0.188942\n0.185916\n1.01628\n0.309498\n\n\n11\nb_rush_hour_bike\n0.0628021\n0.223188\n0.281386\n0.778414"
  },
  {
    "objectID": "discrete-choice.html#interpreting-the-nested-logit-model",
    "href": "discrete-choice.html#interpreting-the-nested-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting the nested logit model",
    "text": "Interpreting the nested logit model\n\n\\(\\mu\\) is 0.76. What is \\(\\theta\\)?\n1 / 0.76 = 1.32\nDo we accept this as a valid nested logit model?"
  },
  {
    "objectID": "discrete-choice.html#exercise-estimate-a-different-nesting-structure-bike-and-walk-together-in-a-nest",
    "href": "discrete-choice.html#exercise-estimate-a-different-nesting-structure-bike-and-walk-together-in-a-nest",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise: estimate a different nesting structure: bike and walk together in a nest",
    "text": "Exercise: estimate a different nesting structure: bike and walk together in a nest\n\n\n\n\n\nflowchart TD\n    mc[Mode choice]\n\n    mc --&gt; Car\n    mc --&gt; Transit\n    mc --&gt; Active\n    Active --&gt; Bike\n    Active --&gt; Walk"
  },
  {
    "objectID": "discrete-choice.html#exercise-answer",
    "href": "discrete-choice.html#exercise-answer",
    "title": "Introduction to Discrete Choice Models",
    "section": "Exercise answer",
    "text": "Exercise answer\n\n\n\nseattle_nested_active.py\n\nimport biogeme.database as db, biogeme.biogeme as bio\nfrom biogeme import models\nfrom biogeme.nests import OneNestForNestedLogit, NestsForNestedLogit\nfrom biogeme.expressions import Beta, Variable\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"data/seattle_trips.csv\") # read data\n\n# Remove people who chose transit despite transit not being available\ndf = df.loc[(df.mode_choice != \"Transit\") | df.transit_available, :]\n\n# create our choice variable - it is coded as text in the data but Biogeme requires\n# it to be numeric\ndf[\"numeric_mode\"] = df.mode_choice.replace({\n    \"Car\": 1,\n    \"Transit\": 2,\n    \"Walk\": 3,\n    \"Bike\": 4\n}).astype(\"int32\")\n\ndata = db.Database(\"Seattle\", pd.get_dummies(df).astype(\"float64\"))\n\n# specify coefficients - all start at zero, have no bounds (None, None), and are\n# estimated rather than fixed (0)\nasc_bike = Beta(\"asc_bike\", 0, None, None, 0)\nb_income_over_100k_bike = Beta(\"b_income_over_100k_bike\", 0, None, None, 0)\nb_rush_hour_bike = Beta(\"b_rush_hour_bike\", 0, None, None, 0)\n\nasc_walk = Beta(\"asc_walk\", 0, None, None, 0)\nb_income_over_100k_walk = Beta(\"b_income_over_100k_walk\", 0, None, None, 0)\nb_rush_hour_walk = Beta(\"b_rush_hour_walk\", 0, None, None, 0)\n\nasc_transit = Beta(\"asc_transit\", 0, None, None, 0)\nb_income_over_100k_transit = Beta(\"b_income_over_100k_transit\", 0, None, None, 0)\nb_rush_hour_transit = Beta(\"b_rush_hour_transit\", 0, None, None, 0)\n\n# we only need one coefficient for travel time\nb_travel_time = Beta(\"b_travel_time\", 0, None, None, 0)\nb_travel_cost = Beta(\"b_travel_cost\", 0, None, None, 0)\n\n# and specify the variables we want to use\nincome_over_100k = Variable(\"income_over_100k\")\nrush_hour = Variable(\"rush_hour\")\n\n# but we need the travel time variables for each mode\ncar_travel_time_mins = Variable(\"car_travel_time_mins\")\nbike_travel_time_mins = Variable(\"bike_travel_time_mins\")\nwalk_travel_time_mins = Variable(\"walk_travel_time_mins\")\ntransit_travel_time_mins = Variable(\"transit_travel_time_mins\")\n\ncar_travel_cost = Variable(\"car_travel_cost\")\nbike_travel_cost = Variable(\"bike_travel_cost\")\nwalk_travel_cost = Variable(\"walk_travel_cost\")\ntransit_travel_cost = Variable(\"transit_travel_cost\")\n\nV = {\n    # Car\n    1: b_travel_time * car_travel_time_mins + b_travel_cost * car_travel_cost,\n\n    # Transit\n    2: asc_transit + b_income_over_100k_transit * income_over_100k + b_rush_hour_transit * rush_hour + b_travel_time * transit_travel_time_mins + b_travel_cost * transit_travel_cost,\n\n    # Walk\n    3: asc_walk + b_income_over_100k_walk * income_over_100k + b_rush_hour_walk * rush_hour + b_travel_time * walk_travel_time_mins + b_travel_cost * walk_travel_cost,\n\n    # Bike\n    4: asc_bike + b_income_over_100k_bike * income_over_100k + b_rush_hour_bike * rush_hour + b_travel_time * bike_travel_time_mins + b_travel_cost * bike_travel_cost\n}\n\nav = {1: 1, 2: Variable(\"transit_available\"), 3: 1, 4: 1}\n\n# specify nests\nmu_active = Beta(\"mu_active\", 1, None, None, 0)\nactive = OneNestForNestedLogit(nest_param=mu_active, list_of_alternatives=[3, 4], name=\"active\")\n\n# If you had multiple nests, you would add them to tuple_of_nests after the comma.\n# there always has to be a comma, even when there is only one nest\nnests = NestsForNestedLogit(choice_set=V.keys(), tuple_of_nests=(active,))\n\nlogprob = models.lognested(V, av, nests, Variable(\"numeric_mode\"))\n\nmodel = bio.BIOGEME(data, logprob)\nmodel.model_name = \"seattle_nested_active\"\nmodel.calculate_null_loglikelihood(av)\nresult = model.estimate()\n\nassert result.algorithm_has_converged\nprint(result.short_summary())\nprint(result.get_estimated_parameters())\n\n\nResults for model seattle_nested_active\nNbr of parameters:      12\nSample size:            8873\nExcluded data:          0\nNull log likelihood:        -11974.36\nFinal log likelihood:       -4515.486\nLikelihood ratio test (null):       14917.74\nRho square (null):          0.623\nRho bar square (null):          0.622\nAkaike Information Criterion:   9054.972\nBayesian Information Criterion: 9140.062\n\n                          Name     Value  Robust std err.  Robust t-stat.  \\\n0                b_travel_time -0.055173         0.001863      -29.609326   \n1                b_travel_cost -0.101328         0.010511       -9.639797   \n2                  asc_transit -1.210504         0.080964      -14.951203   \n3   b_income_over_100k_transit -1.511167         0.152876       -9.884944   \n4          b_rush_hour_transit  0.148930         0.140581        1.059390   \n5                     asc_walk  0.046756         0.057345        0.815348   \n6      b_income_over_100k_walk  0.420584         0.065411        6.429860   \n7             b_rush_hour_walk -0.194607         0.088373       -2.202104   \n8                    mu_active  1.071948         0.084551       12.678100   \n9                     asc_bike -3.288892         0.214599      -15.325765   \n10     b_income_over_100k_bike  0.210226         0.175634        1.196954   \n11            b_rush_hour_bike  0.020700         0.215682        0.095974   \n\n    Robust p-value  \n0     0.000000e+00  \n1     0.000000e+00  \n2     0.000000e+00  \n3     0.000000e+00  \n4     2.894223e-01  \n5     4.148733e-01  \n6     1.277216e-10  \n7     2.765795e-02  \n8     0.000000e+00  \n9     0.000000e+00  \n10    2.313244e-01  \n11    9.235412e-01"
  },
  {
    "objectID": "discrete-choice.html#nested-logit-results-again",
    "href": "discrete-choice.html#nested-logit-results-again",
    "title": "Introduction to Discrete Choice Models",
    "section": "Nested logit results, again",
    "text": "Nested logit results, again\n\n\n\n\n\n\nName\nValue\nRobust std err.\nRobust t-stat.\nRobust p-value\n\n\n\n\n0\nb_travel_time\n-0.055173\n0.00186336\n-29.6093\n0\n\n\n1\nb_travel_cost\n-0.101328\n0.0105114\n-9.6398\n0\n\n\n2\nasc_transit\n-1.2105\n0.0809636\n-14.9512\n0\n\n\n3\nb_income_over_100k_transit\n-1.51117\n0.152876\n-9.88494\n0\n\n\n4\nb_rush_hour_transit\n0.14893\n0.140581\n1.05939\n0.289422\n\n\n5\nasc_walk\n0.0467565\n0.0573455\n0.815348\n0.414873\n\n\n6\nb_income_over_100k_walk\n0.420584\n0.065411\n6.42986\n1.27722e-10\n\n\n7\nb_rush_hour_walk\n-0.194607\n0.0883733\n-2.2021\n0.0276579\n\n\n8\nmu_active\n1.07195\n0.0845512\n12.6781\n0\n\n\n9\nasc_bike\n-3.28889\n0.214599\n-15.3258\n0\n\n\n10\nb_income_over_100k_bike\n0.210226\n0.175634\n1.19695\n0.231324\n\n\n11\nb_rush_hour_bike\n0.0206998\n0.215682\n0.0959741\n0.923541"
  },
  {
    "objectID": "discrete-choice.html#interpreting-the-nested-logit-model-again",
    "href": "discrete-choice.html#interpreting-the-nested-logit-model-again",
    "title": "Introduction to Discrete Choice Models",
    "section": "Interpreting the nested logit model, again",
    "text": "Interpreting the nested logit model, again\n\n\\(\\mu\\) is 1.07. What is \\(\\theta\\)?\n1 / 1.07 = 0.93\nDo we accept this as a valid nested logit model?\nWhat does this mean about the error correlation between walking and biking?"
  },
  {
    "objectID": "discrete-choice.html#testing-the-nested-logit-model",
    "href": "discrete-choice.html#testing-the-nested-logit-model",
    "title": "Introduction to Discrete Choice Models",
    "section": "Testing the nested logit model",
    "text": "Testing the nested logit model\n\nIf \\(\\theta\\) were 1, the results would be the same as the multinomial logit model, and we wouldn’t need a nested logit model\n0.93 isn’t that different from 1\nHow do we know we need a nested logit model?\nWhen there’s just one nest we can use a standard \\(z\\)-test and \\(p\\)-value\n\nMuch software doesn’t do the right test, though—we need to test that the nesting parameter (\\(\\theta\\) or \\(\\mu\\)) is different from one, not zero like most coefficients\nIn our case, \\(\\mu\\) is just under one standard error above 1, so is not statistically significant\nWe can reject this nesting structure as not being superior to multinomial logit, though there may be other nesting structures that are"
  },
  {
    "objectID": "discrete-choice.html#the-likelihood-ratio-test",
    "href": "discrete-choice.html#the-likelihood-ratio-test",
    "title": "Introduction to Discrete Choice Models",
    "section": "The likelihood-ratio test",
    "text": "The likelihood-ratio test\n\nThe standard test for a nested-logit model versus a multinomial logit model is a likelihood-ratio test\nThis works even when there are multiple nests\nThe likelihood-ratio is a statistical test for whether the overall improvement in the model justifies the added parameters\nThe test statistic is \\(-2 (\\ell_{mnl} - \\ell_{nl})\\)\n\nThis is called a likelihood-ratio test because a difference in logarithms is equivalent to division\n\nIt has a \\(\\chi^2\\) distribution (yay another distribution)\nThe number of degrees of freedom is the number of nests you have"
  },
  {
    "objectID": "discrete-choice.html#the-likelihood-ratio-test-1",
    "href": "discrete-choice.html#the-likelihood-ratio-test-1",
    "title": "Introduction to Discrete Choice Models",
    "section": "The likelihood-ratio test",
    "text": "The likelihood-ratio test\n\nOur log-likelihoods are:\n\nMNL: -4515.951\nNL: -4515.486\n\nDifference: -0.465\nTest statistic: 0.930\n\\(\\chi^2\\) p-value: 0.34\nCan we conclude that the NL model is superior to the MNL?"
  },
  {
    "objectID": "discrete-choice.html#the-likelihood-ratio-test-in-code",
    "href": "discrete-choice.html#the-likelihood-ratio-test-in-code",
    "title": "Introduction to Discrete Choice Models",
    "section": "The likelihood-ratio test, in code",
    "text": "The likelihood-ratio test, in code\n\nimport scipy\n\n# 0.930 is our test statistic, 1 is the number of degrees of freedom\n# sf is the \"survival function\", 1 minus the cumulative distribution function\nscipy.stats.chi2.sf(0.930, 1)\n\nnp.float64(0.33486292507904936)"
  },
  {
    "objectID": "discrete-choice.html#the-likelihood-ratio-test-more-generally",
    "href": "discrete-choice.html#the-likelihood-ratio-test-more-generally",
    "title": "Introduction to Discrete Choice Models",
    "section": "The likelihood-ratio test, more generally",
    "text": "The likelihood-ratio test, more generally\n\nThe likelihood-ratio test can be used for any two “nested” models\n\nConfusingly, not the same as a nested-logit model\nNested models mean that the more complex model can be transformed into the simple model by restricting some coefficients\nFor instance, a nested logit can be transformed into an MNL by restricting the \\(\\theta\\)’s to 1\nOr two multinomial logit models where one has a few added coefficients, the more complex model is equivalent to the simpler model if those coefficients are restricted to zero\nThe number of degrees of freedom is just the number of restrictions"
  },
  {
    "objectID": "discrete-choice.html#multiple-nesting-levels",
    "href": "discrete-choice.html#multiple-nesting-levels",
    "title": "Introduction to Discrete Choice Models",
    "section": "Multiple nesting levels",
    "text": "Multiple nesting levels\n\nThe nested logit we just estimated only had one nest\nYou can estimate nested logit models with multiple nests, and even multiple levels of nests\nFor instance, you could estimate this model\n\n\n\n\n\n\nflowchart TD\n    mc[Mode choice]\n    da[Drive alone]\n\n    mc --&gt; Motorized\n    mc --&gt; Nonmotorized\n    Motorized --&gt; Car\n    Motorized --&gt; Transit\n    Car --&gt; da\n    Car --&gt; Carpool\n    Nonmotorized --&gt; Walk\n    Nonmotorized --&gt; Bike\n\n\n\n\n\n\n\nWe will have one \\(\\theta\\) for each nest\n\\(\\theta\\)’s for top-level nests should be between 0 and 1\n\\(\\theta\\)’s for subnests should be between 0 and the \\(\\theta\\) in the next higher level nest\n\nAnd a \\(z\\)-test should be relative to the \\(\\theta\\) in the next higher nest, or just use a likelihood-ratio test\n\nBiogeme can’t directly estimate this type of model; larch (Python) or Apollo (R) can."
  },
  {
    "objectID": "discrete-choice.html#going-deeper-scale-parameters-in-nested-logit",
    "href": "discrete-choice.html#going-deeper-scale-parameters-in-nested-logit",
    "title": "Introduction to Discrete Choice Models",
    "section": "Going deeper: scale parameters in nested logit",
    "text": "Going deeper: scale parameters in nested logit\n\nAnother way to think about the nested logit model is that it allows an error term that varies between nests/top-level alternatives and another that varies within each nest\nSo, for instance, the utilities of biking and walking in our nested logit model above are\n\n\\[\n\\begin{align}\nU_{bike} &= V_{bike} + \\epsilon_{nonmotorized} + \\epsilon_{bike} \\\\\nU_{walk} &= V_{walk} + \\epsilon_{nonmotorized} + \\epsilon_{walk}\n\\end{align}\n\\]\n\nWhat \\(\\theta\\) actually represents is the ratio of the scales of the error terms at the top-level portion of the model and in the nest\nAs \\(\\theta\\) approaches 1, the scales of the error approach equality. When they are equal, the within nest variation is contributing all of the error term, and there is no shared unobserved variation."
  },
  {
    "objectID": "discrete-choice.html#estimation-challenges-with-the-nested-logit-local-optima-i",
    "href": "discrete-choice.html#estimation-challenges-with-the-nested-logit-local-optima-i",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimation challenges with the nested logit: local optima I",
    "text": "Estimation challenges with the nested logit: local optima I\n\n\n\nBoth multinomial logit and nested logit models are solved using maximum likelihood, which is an iterative process of updating coefficients to improve the loglikelihood\nThe process adjusts the coefficients until it finds a point where it cannot improve the likelihood further through small changes to the coefficients\nThe multinomial logit model is globally convex—there is only one place where moving the coefficients does not improve the loglikelihood—at the solution"
  },
  {
    "objectID": "discrete-choice.html#estimation-challenges-with-the-nested-logit-local-optima-ii",
    "href": "discrete-choice.html#estimation-challenges-with-the-nested-logit-local-optima-ii",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimation challenges with the nested logit: local optima II",
    "text": "Estimation challenges with the nested logit: local optima II\n\n\n\nThe nested logit model is not—there may be local optima where moving the coefficients a small amount cannot improve the loglikelihood, even if moving them quite a bit might find a set of coefficients with a better loglikelihood\nYou can experiment with this by adjusting the starting values of the coefficients—the first number in the call to Beta()\nKoppelman & Bhat (2006) A Self Instructing Course in Mode Choice Modeling: Multinomial and Nested Logit Models recommends trying starting with \\(\\theta = 0.5\\) (\\(\\mu = 2\\)) as this may lead to a higher likelihood of finding a valid nested logit solution"
  },
  {
    "objectID": "discrete-choice.html#estimation-challenges-with-logit-models-generally-scaling",
    "href": "discrete-choice.html#estimation-challenges-with-logit-models-generally-scaling",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimation challenges with logit models generally: scaling",
    "text": "Estimation challenges with logit models generally: scaling\n\nIf your variables have very different scales (e.g. income in dollars and immigrant status as a binary variable), you may run into numerical issues\nWe would expect the coefficient on income to be very small, for example, if it is per dollar\nIt is often wise to scale variables to not have wildly different scales—e.g. put income in thousands or tens of thousands"
  },
  {
    "objectID": "discrete-choice.html#estimation-challenges-with-logit-models-generally-convergence-and-identification",
    "href": "discrete-choice.html#estimation-challenges-with-logit-models-generally-convergence-and-identification",
    "title": "Introduction to Discrete Choice Models",
    "section": "Estimation challenges with logit models generally: convergence and identification",
    "text": "Estimation challenges with logit models generally: convergence and identification\n\nIn each of the models above, we checked for convergence\nIf the model did not converge, it means Biogeme did not find a solution\nThis usually means an identification issue—for instance, multicollinearity, not excluding one ASC, etc.\nThere may also be cases where there is an idenfication issue that is indicated but unexpected coefficients, non-finite likelihoods, etc.—if anything looks off, identification is a good thing to think about\nSome identification issues may not be apparent in results at all"
  },
  {
    "objectID": "discrete-choice.html#conclusion",
    "href": "discrete-choice.html#conclusion",
    "title": "Introduction to Discrete Choice Models",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe covered the binary, multinomial, and nested logit models\nThe multinomial logit model is used for multiple outcomes; the nested logit is used when you are concerned about independence of irrelevant alternatives\nWe estimated models using the Biogeme package, other packages are larch (Python) and apollo (R)"
  },
  {
    "objectID": "discrete-choice.html#questions",
    "href": "discrete-choice.html#questions",
    "title": "Introduction to Discrete Choice Models",
    "section": "Questions?",
    "text": "Questions?\nOther resources:\n\nThe quintessential reference, albeit very technical: Ben-Akiva & Lerman (1985) Discrete Choice Analysis: Theory and Application to Travel Demand\nA more practitioner-focused text (free e-book): Koppelman & Bhat (2006) A Self Instructing Course in Mode Choice Modeling: Multinomial and Nested Logit Models\nDesigning choice experiments: Bliemer & Rose (2024) Designing and conducting stated choice experiments Handbook of Choice Modelling\n\nPart of my job is one-on-one statistical support. Email me: mwbc@unc.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Odum Discrete Choice Modeling class",
    "section": "",
    "text": "Example code and data\nSlides"
  }
]