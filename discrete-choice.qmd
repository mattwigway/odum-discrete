---
title: Introduction to Discrete Choice Models
author: Matt Bhagat-Conway
institute: Odum Institute<br/>University of North Carolina at Chapel Hill
format:
    revealjs:
        theme: [default, unc.scss]
        width: 1920
        height: 1080
        logo: UNC_logo_RGB.png
        slide-number: true
        html-math-method: katex
jupyter: odum-discrete
execute:
    eval: true
    echo: true
    output: true
    cache: true
---

```{python}
#| include: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from IPython.display import Markdown, HTML
import tabulate
import html
plt.rcParams.update({'font.size': 36})
```

## About me

- Assistant professor of City and Regional Planning
- Research focus: transportation modeling and simulation
    - Heavy use of discrete choice models
- PhD in Geography from Arizona State
- Three years experience as transportation modeling software developer

## Before we go any further

- Install [Anaconda](https://www.anaconda.com/) and [Biogeme](https://biogeme.epfl.ch/#install)
- Slides, code, and data at [https://projects.indicatrix.org/odum-discrete/](https://projects.indicatrix.org/odum-discrete/)

## What are discrete choice models?

- Regression models of discrete outcomes
- Most are consistent with _random utility theory_, an theory of economically rational decisionmaking

## What are discrete choice models used for?

- Economic analysis
    - Particularly willingness-to-pay/valuation
- Marketing
- Healthcare
- Transportation planning

## The simplest discrete choice model: binary logistic regression

- You may have also just heard this called "logistic regression"
- If you've taken regression you're probably familiar with this model
- It is used for binary (two outcome) variables, e.g.
    - fraudulent or not
    - work in person or work at home
    - in foreclosure or not
    - etc.

## The binary logistic regression model: conceptually

- We are never going to be 100% sure of one outcome or the other, so we want to model the _probability_ of the outcomes
- We could just assing 1 to one outcome and 0 to the other, and run a linear regression model
- This is the _linear probability model_

## The problem with the linear probability model

- Predicted values are not constrained between 0 and 1

```{python}
#| echo: false
#| fig-alt: A plot of a simple linear regression showing a predictor on the x axis and a predicted probability on the y axis, and a positive linear relationship with the predicted probability ranging from -0.25 to 1.25.
df = pd.DataFrame(dict(x=np.arange(-1, 5, 0.01)))
df['y'] = df.x * 0.25
plt.figure(figsize=(24, 8))
plt.fill_between([-10, 10], 0, 1, color="lightgray")
plt.plot(df.x, df.y)
plt.xlim(-1, 5)
plt.xticks([])
plt.xlabel("Predictor")
plt.ylabel("Predicted probability");
```

## Binary logistic regression

- Instead of predicting the probability, we predict the log-odds (aka logit)

```{python}
#| echo: false
#| fig-alt: A plot of a simple linear regression showing a predictor on the x axis and a predicted probability on the y axis, and a positive linear relationship with the predicted probability ranging from -0.25 to 1.25.
df = pd.DataFrame(dict(x=np.arange(-20, 20, 0.1)))
df['y'] = df.x * 0.25
df['p'] = 1 / (1 + np.exp(-df.y))
plt.figure(figsize=(24, 8))
plt.fill_between([-20, 20], 0, 1, color="lightgray")
plt.plot(df.x, df.p)
plt.xlim(-20, 20)
plt.xticks([])
plt.xlabel("Predictor")
plt.ylabel("Predicted probability");
```

## Binary logistic regression: the math

$$
y^* = \alpha + \beta_1 x_1 + \beta_2 x_2 \cdots + \epsilon
$$

$$
\epsilon \thicksim \mathrm{Logistic}
$$
$$
p(y = 1) = p(y^* > 0) = \frac{e^{y^*}}{1 + e^{y^*}}
$$

## Interpreting a binary logistic regression model

```{python}
#| include: false
cfwfh = pd.read_csv("data/wfh_prediction_covidfuture.csv")
model = sm.Logit(
    cfwfh.wfh_expectation,
    sm.add_constant(pd.get_dummies(cfwfh[["age", "gender", "college"]], drop_first=True)).astype("float64")
).fit()

modelRes = pd.DataFrame([
    model.params.rename("Coefficient").round(3),
    model.bse.rename("Std. Err.").round(3),
    model.tvalues.rename("$t$").round(3),
    model.pvalues.rename("p").round(3)
]).transpose().rename({
    "const": "Intercept",
    "age": "Age",
    "college": "College degree",
    "gender_Male": "Male"
})

modelRes = pd.concat([modelRes,
    pd.DataFrame(pd.Series(
        [model.nobs, model.llf, model.llnull, model.prsquared],
        index=["n", "Log-likelihood", "Log-likelihood at constants", "Pseudo-RÂ²"]
    ).rename("p"))]
)
```

```{python}
#| echo: false
Markdown(tabulate.tabulate(modelRes.fillna(""), headers=modelRes.columns))
```

::: {.incremental}
- Are older people more likely to be able to WFH?
- Are college-educated people?
- Are men?
:::

## Odds ratios
```{python}
#| echo: false
Markdown(tabulate.tabulate(modelRes.fillna(""), headers=modelRes.columns))
```

::: {.incremental}
- What does a "0.93 increase in log-odds" mean anyways?
- Because this is hard to interpret, we often use an _odds ratio_ instead
- This is just the exponentiation of the coefficient
:::

## Interpreting odds ratios

```{python}
#| echo: false
modelRes["Odds ratio"] = np.exp(modelRes.Coefficient).round(3)
oddRes = modelRes[["Odds ratio", "$t$", "p"]]
Markdown(tabulate.tabulate(oddRes.fillna(""), headers=oddRes.columns))
```

::: {.incremental}
- Odds ratios measure the change in the _odds_ of an event happening
    - Odds is the probability of something happening divided by the probability of it not happening
    - e.g. 5 to 1 odds means that out of 6 trials something will happen 5 times on average
- They are multiplicative, so an odds ratio of 1.287 means that men have 28.7% higher odds of being able to work from home
- What does an odds ratio of 0.977 mean?
    - Each additional year of age is associated with 2.3% _lower_ odds of being able to work from home
:::

## Odds ratios are not risk ratios

- Odds ratios are often misinterpreted as _risk ratios_, the percent change in the probability
- Men have 28.7% higher _odds_ of being able to work from home, not 28.7% higher probability
    - Many people say something like 28.7% more likely, but this is unclear that it refers to odds rather than probability

## Running a logistic regression in Python
```{python}
#| filename: logistic_basic.py
#| include: true
#| echo: true
import pandas as pd, statsmodels.api as sm

data = pd.read_csv("data/wfh_prediction_covidfuture.csv")

model = sm.Logit(
    data.wfh_expectation, # dependent variables
    sm.add_constant( # add an intercept to the dataframe
        pd.get_dummies( # convert gender to a dummy variable
            data[["age", "gender", "college"]],
        )
        .drop(columns=["gender_Female"]) # drop one category from dummy variable
        .astype("float64") # convert everything to numeric
    )
)

result = model.fit()
result.summary()
```

## Exercise: add the `black` and `hispanic` variables to the model

```{python}
#| filename: logistic_black_hispanic.py
#| include: true
#| echo: true
import pandas as pd, statsmodels.api as sm

data = pd.read_csv("data/wfh_prediction_covidfuture.csv")

model = sm.Logit(
    data.wfh_expectation, # dependent variables
    sm.add_constant( # add an intercept to the dataframe
        pd.get_dummies( # convert gender to a dummy variable
            data[["age", "gender", "college", "black", "hispanic"]],
        )
        .drop(columns=["gender_Female"]) # drop one category from dummy variable
        .astype("float64") # convert everything to numeric
    )
)

result = model.fit()
result.summary()
```

::: {.incremental}
- Is either statistically significant?
:::

## An alternate conceptualization of the logistic regression model: random utility theory

- Random utility theory posits that choices have "utility" or value
- When faced with a set of options, individuals will choose the option with the highest utility
- We don't observe everything that goes into utility, so there is random error

## Logistic regression as a random utility model

- We define the systematic utility of one alternative (e.g. working from home) as
$$
V_{wfh} = \alpha + \beta_1 x_1 + \beta_2 x_2 \cdots
$$

The full utility has an error term

$$
U_{wfh} = V_{wfh} + \epsilon_{wfh}
$$

- Only differences in utility are meaningful, so we can arbitrarily define the systematic portion of the utility of the other option as zero

$$
U_{notwfh} = 0 + \epsilon{notwfh}
$$

The probability of working from home then becomes

$$
P\left(U_{wfh} > U_{notwfh}\right)
$$

## Error term distribution

::: {.incremental}
- We assume the $\epsilon$'s are Gumbel-distributed
- The Gumbel distribution is an approximation of the normal distribution with some attractive properties
    - More on this later
- The important property at the moment is that the difference of two Gumbel-distributed variables is Logistic-distributed
    - Just like the error term in our logistic regression model
:::

## Showing the equivalence to the binary logistic regression model I

We have

$$
P\left(U_{wfh} > U_{notwfh}\right)
$$

. . .

which we can re-write as 

$$
P\left(V_{wfh} + \epsilon_{wfh} - (0 + \epsilon_{notwfh}) > 0\right)
$$

## Showing the equivalence to the binary logistic regression model II

- Since the difference of our Gumbel-distributed error terms is Logistic-distributed, we have

$$
P\left(V_{wfh} + \epsilon_{\Delta} > 0\right)
$$

with $\epsilon_\Delta$ Logistic-distributed, which is the same as our formula for logistic regression.

## Estimating our binary logistic regression as a random utility model

- We're going to specify our logistic regression model as a random utility model
- We're going to use the [Biogeme](https://biogeme.epfl.ch) package to do so

## Estimation code

```{python}
#| filename: biogeme_binary.py
#| output: false
import biogeme.database as db, biogeme.biogeme as bio
from biogeme import models
from biogeme.expressions import Beta, Variable
import pandas as pd

df = pd.read_csv("data/wfh_prediction_covidfuture.csv") # read data
data = db.Database("WFH", 
    pd.get_dummies(df)
        .drop(columns="gender_Female")
        .astype("float64")
) # convert to Biogeme format

# specify coefficients - all start at zero, have no bounds (None, None), and are
# estimated rather than fixed (0)
alpha = Beta("Intercept", 0, None, None, 0)
b_age = Beta("b_age", 0, None, None, 0)
b_college = Beta("b_college", 0, None, None, 0)
b_male = Beta("b_male", 0, None, None, 0)

# and specify the variables we want to use
age = Variable("age")
college = Variable("college")
male = Variable("gender_Male")

# specify utility functions
V = {
    # outcome 1 is WFH
    1: (alpha + b_age * age + b_college * college + b_male * male),
    # outcome 0 is non-WFH
    0: 0
}

# we also need to define the availability (more on this later)
av = {0: 1, 1: 1}

# now, set up what type of model we want (logit), what the utility functions are,
# what the availability variables are, and what the dependent variable is
logprob = models.loglogit(V, av, Variable("wfh_expectation"))

model = bio.BIOGEME(data, logprob)
model.modelName = "wfh_logit"
model.calculateNullLoglikelihood(av)
result = model.estimate()
print(result.shortSummary())
print(result.getEstimatedParameters())
```

## Result

```{python}
#| echo: false
print(result.shortSummary())
print(result.getEstimatedParameters())
```

## How does the result compare to the original model we estimated?

```{python}
#| echo: false
Markdown(tabulate.tabulate(modelRes.fillna(""), headers=modelRes.columns))
```

::: {.incremental}
- It should be the same
:::

## Detailed result

Stored in "wfh_logit.html", or some variationâlook for most recent.

```{python}
#| echo: false
def fixHtmlFile(fn):
    with open(fn) as f:
        htxt = f.read()

    # this http script causes load issues on modern browsers
    htxt = htxt.replace('<script src="http://transp-or.epfl.ch/biogeme/sorttable.js"></script>', '')

    with open(fn, "w") as f:
        f.write(htxt)

fixHtmlFile(result.data.htmlFileName)
# https://stackoverflow.com/questions/1061697, https://github.com/quarto-dev/quarto-cli/issues/718
HTML('<iframe width="1850" height="800px" src="{}" data-external="1"></iframe>'.format(html.escape(result.data.htmlFileName)))
```


## Exercise

- Add the `black` and `hispanic` variables to the model, confirm you get the same results you did before

```{python}
#| filename: biogeme_binary_race.py
#| echo: false
import biogeme.database as db, biogeme.biogeme as bio
from biogeme import models
from biogeme.expressions import Beta, Variable
import pandas as pd

df = pd.read_csv("data/wfh_prediction_covidfuture.csv") # read data
data = db.Database("WFH", 
    pd.get_dummies(df)
        .drop(columns="gender_Female")
        .astype("float64")
) # convert to Biogeme format

# specify coefficients - all start at zero, have no bounds (None, None), and are
# estimated rather than fixed (0)
alpha = Beta("Intercept", 0, None, None, 0)
b_age = Beta("b_age", 0, None, None, 0)
b_college = Beta("b_college", 0, None, None, 0)
b_male = Beta("b_male", 0, None, None, 0)
b_black = Beta("b_black", 0, None, None, 0)
b_hispanic = Beta("b_hispanic", 0, None, None, 0)

# and specify the variables we want to use
age = Variable("age")
college = Variable("college")
male = Variable("gender_Male")
black = Variable("black")
hispanic = Variable("hispanic")

# specify utility functions
V = {
    # outcome 1 is WFH
    1: (alpha + b_age * age + b_college * college + b_male * male +
        b_black * black + b_hispanic * hispanic),
    # outcome 0 is non-WFH
    0: 0
}

# we also need to define the availability (more on this later)
av = {0: 1, 1: 1}

# now, set up what type of model we want (logit), what the utility functions are,
# what the availability variables are, and what the dependent variable is
logprob = models.loglogit(V, av, Variable("wfh_expectation"))

model = bio.BIOGEME(data, logprob)
model.modelName = "wfh_logit_race"
model.calculateNullLoglikelihood(av)
result = model.estimate()
print(result.shortSummary())
print(result.getEstimatedParameters())
```

## Results

```{python}
#| echo: false
print(result.shortSummary())
print(result.getEstimatedParameters())
```
